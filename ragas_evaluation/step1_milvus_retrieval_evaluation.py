#!/usr/bin/env python3
"""
ë‹¨ê³„ë³„ RAG í‰ê°€: 1ë‹¨ê³„ - Milvus ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
PyTorch ì˜ì¡´ì„± ì—†ì´ í˜„ì¬ ì‘ë™í•˜ëŠ” Milvus ê²€ìƒ‰ ê¸°ëŠ¥ìœ¼ë¡œ í‰ê°€ ì§„í–‰
"""

import sys
import os
import json
import time
import random
from datetime import datetime
from typing import List, Dict, Any, Tuple

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def print_section_header(title):
    print(f"\n{'='*80}")
    print(f"  {title}")
    print(f"{'='*80}")

def load_evaluation_dataset():
    """í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ"""
    try:
        with open('src/evaluation/data/insurance_eval_dataset.json', 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        questions = dataset.get('questions', [])
        print(f"[OK] í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: {len(questions)}ê°œ ì§ˆë¬¸")
        return questions
        
    except Exception as e:
        print(f"[FAIL] í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

def create_simple_query_embeddings(questions: List[Dict]) -> List[Tuple[str, List[float], str]]:
    """
    ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ì‹¤ì œ ì„ë² ë”© ëŒ€ì‹  ì„ì‹œ ë²¡í„° ì‚¬ìš©)
    ì‹¤ì œë¡œëŠ” ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ëœë¤ ë²¡í„° ì‚¬ìš©
    """
    print("[INFO] ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘...")
    
    query_embeddings = []
    
    for i, q in enumerate(questions[:10]):  # ì²˜ìŒ 10ê°œë§Œ í…ŒìŠ¤íŠ¸
        question_text = q.get('text', '')
        question_id = q.get('id', f'q_{i}')
        
        # ì„ì‹œ 768ì°¨ì› ë²¡í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©í•´ì•¼ í•¨)
        # í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì•½ê°„ì˜ íŒ¨í„´ ë¶€ì—¬
        embedding = [random.random() for _ in range(768)]
        
        # ë³´í—˜, ê³„ì•½ ë“± í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ íŠ¹ì • ì°¨ì›ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
        if any(keyword in question_text for keyword in ['ë³´í—˜', 'ê³„ì•½', 'ë³´ì¥']):
            for j in range(0, 100):  # ì²˜ìŒ 100ê°œ ì°¨ì›ì— ê°€ì¤‘ì¹˜
                embedding[j] += 0.3
        
        if any(keyword in question_text for keyword in ['ì§€ê¸‰', 'ë³´í—˜ê¸ˆ', 'ì²­êµ¬']):
            for j in range(100, 200):  # ë‹¤ìŒ 100ê°œ ì°¨ì›ì— ê°€ì¤‘ì¹˜
                embedding[j] += 0.3
        
        # ì •ê·œí™”
        norm = sum(x*x for x in embedding) ** 0.5
        if norm > 0:
            embedding = [x/norm for x in embedding]
        
        query_embeddings.append((question_id, embedding, question_text))
    
    print(f"[OK] {len(query_embeddings)}ê°œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    return query_embeddings

def evaluate_milvus_retrieval(query_embeddings: List[Tuple[str, List[float], str]]) -> Dict[str, Any]:
    """Milvus ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€"""
    print_section_header("Milvus ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€")
    
    try:
        from src.vectordb.milvus_client import MilvusClient
        from src.utils.config import Config
        
        config = Config()
        client = MilvusClient(config)
        
        if not client.is_connected():
            print("âŒ Milvus ì—°ê²° ì‹¤íŒ¨")
            return {}
        
        print("âœ… Milvus ì—°ê²° ì„±ê³µ")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ í™•ì¸
        collections = client.list_collections()
        target_collection = "insurance_ko_sroberta"  # ê°€ì¥ ìµœì‹  ëª¨ë¸ ì»¬ë ‰ì…˜
        
        if target_collection not in collections:
            print(f"âŒ ëŒ€ìƒ ì»¬ë ‰ì…˜ '{target_collection}' ì—†ìŒ")
            return {}
        
        print(f"âœ… ëŒ€ìƒ ì»¬ë ‰ì…˜: {target_collection}")
        
        evaluation_results = {
            "collection_name": target_collection,
            "total_queries": len(query_embeddings),
            "search_results": [],
            "performance_metrics": {},
            "timestamp": datetime.now().isoformat()
        }
        
        total_search_time = 0
        successful_searches = 0
        
        for i, (q_id, embedding, question_text) in enumerate(query_embeddings):
            print(f"\nğŸ” ê²€ìƒ‰ {i+1}/{len(query_embeddings)}: {q_id}")
            print(f"   ì§ˆë¬¸: {question_text[:100]}...")
            
            try:
                start_time = time.time()
                
                # ê²€ìƒ‰ ì‹¤í–‰
                search_results = client.search(
                    collection_name=target_collection,
                    query_vector=embedding,
                    top_k=5,
                    output_fields=["id", "text", "doc_id", "source", "page_num", "chunk_type"]
                )
                
                search_time = time.time() - start_time
                total_search_time += search_time
                
                if search_results:
                    successful_searches += 1
                    print(f"   âœ… {len(search_results)}ê°œ ê²°ê³¼ ë°˜í™˜ (ì†Œìš”ì‹œê°„: {search_time:.3f}ì´ˆ)")
                    
                    # ê²°ê³¼ ë¶„ì„
                    result_analysis = analyze_search_results(search_results, question_text)
                    
                    evaluation_results["search_results"].append({
                        "question_id": q_id,
                        "question": question_text,
                        "search_time": search_time,
                        "num_results": len(search_results),
                        "top_result": {
                            "score": search_results[0].get("score", 0),
                            "text": search_results[0].get("text", "")[:200],
                            "chunk_type": search_results[0].get("chunk_type", ""),
                            "page_num": search_results[0].get("page_num", -1)
                        },
                        "analysis": result_analysis
                    })
                    
                        print(f"     [{j+1}] Score: {score:.4f}, Type: {chunk_type}")
                        print(f"         Text: {text}...")
                
                else:
                    print(f"   [FAIL] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    
            except Exception as search_error:
                print(f"   [ERROR] ê²€ìƒ‰ ì˜¤ë¥˜: {search_error}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        evaluation_results["performance_metrics"] = {
            "success_rate": successful_searches / len(query_embeddings),
            "average_search_time": total_search_time / len(query_embeddings),
            "total_search_time": total_search_time,
            "successful_searches": successful_searches,
            "failed_searches": len(query_embeddings) - successful_searches
        }
        
        client.close()
        print(f"\n[OK] ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ")
        return evaluation_results
        
    except Exception as e:
        print(f"[FAIL] Milvus ê²€ìƒ‰ í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {}

def analyze_search_results(results: List[Dict], question: str) -> Dict[str, Any]:
    """ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„"""
    if not results:
        return {"relevance_indicators": [], "chunk_type_distribution": {}, "score_statistics": {}}
    
    # ê´€ë ¨ì„± ì§€í‘œ ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)
    question_keywords = extract_keywords(question)
    relevance_indicators = []
    
    for result in results:
        text = result.get("text", "").lower()
        relevance_score = sum(1 for keyword in question_keywords if keyword.lower() in text)
        relevance_indicators.append(relevance_score)
    
    # ì²­í¬ íƒ€ì… ë¶„í¬
    chunk_types = {}
    for result in results:
        chunk_type = result.get("chunk_type", "unknown")
        chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
    
    # ì ìˆ˜ í†µê³„
    scores = [result.get("score", 0) for result in results]
    score_stats = {
        "min_score": min(scores) if scores else 0,
        "max_score": max(scores) if scores else 0,
        "avg_score": sum(scores) / len(scores) if scores else 0,
        "score_range": max(scores) - min(scores) if scores else 0
    }
    
    return {
        "relevance_indicators": relevance_indicators,
        "chunk_type_distribution": chunk_types,
        "score_statistics": score_stats,
        "avg_relevance": sum(relevance_indicators) / len(relevance_indicators) if relevance_indicators else 0
    }

def extract_keywords(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    insurance_keywords = [
        'ë³´í—˜', 'ê³„ì•½', 'ë³´ì¥', 'ì§€ê¸‰', 'ì²­êµ¬', 'ë³´í—˜ê¸ˆ', 'ë³´í—˜ë£Œ', 'í”¼ë³´í—˜ì', 'ê³„ì•½ì',
        'íŠ¹ì•½', 'í•´ì§€', 'í•´ì•½', 'ë‚©ì…', 'ê¸‰ì—¬ê¸ˆ', 'ìˆ˜ìµì', 'ì§„ë‹¨', 'ì§ˆë³‘', 'ìƒí•´', 
        'ì¬í•´', 'ì¥í•´', 'ì‚¬ë§', 'ë§Œê¸°', 'ê°±ì‹ ', 'ë¶€í™œ', 'ë©´ì œ'
    ]
    
    found_keywords = []
    text_lower = text.lower()
    
    for keyword in insurance_keywords:
        if keyword in text_lower:
            found_keywords.append(keyword)
    
    return found_keywords

def save_evaluation_results(results: Dict[str, Any], filename: str = None):
    """í‰ê°€ ê²°ê³¼ ì €ì¥"""
    if not results:
        print("[FAIL] ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"evaluation_results/step1_milvus_retrieval_{timestamp}.json"
    
    try:
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"[OK] í‰ê°€ ê²°ê³¼ ì €ì¥: {filename}")
        
    except Exception as e:
        print(f"[FAIL] ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def print_evaluation_summary(results: Dict[str, Any]):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    if not results:
        return
    
    print_section_header("í‰ê°€ ê²°ê³¼ ìš”ì•½")
    
    metrics = results.get("performance_metrics", {})
    
    print(f"[METRICS] ê²€ìƒ‰ ì„±ëŠ¥ í†µê³„:")
    print(f"   - ì´ ì¿¼ë¦¬ ìˆ˜: {results.get('total_queries', 0)}ê°œ")
    print(f"   - ì„±ê³µë¥ : {metrics.get('success_rate', 0):.1%}")
    print(f"   - í‰ê·  ê²€ìƒ‰ ì‹œê°„: {metrics.get('average_search_time', 0):.3f}ì´ˆ")
    print(f"   - ì„±ê³µí•œ ê²€ìƒ‰: {metrics.get('successful_searches', 0)}ê°œ")
    print(f"   - ì‹¤íŒ¨í•œ ê²€ìƒ‰: {metrics.get('failed_searches', 0)}ê°œ")
    
    # ì²­í¬ íƒ€ì… ë¶„ì„
    chunk_type_summary = {}
    relevance_scores = []
    
    for result in results.get("search_results", []):
        analysis = result.get("analysis", {})
        
        # ì²­í¬ íƒ€ì… ì§‘ê³„
        for chunk_type, count in analysis.get("chunk_type_distribution", {}).items():
            chunk_type_summary[chunk_type] = chunk_type_summary.get(chunk_type, 0) + count
        
        # ê´€ë ¨ì„± ì ìˆ˜ ìˆ˜ì§‘
        avg_relevance = analysis.get("avg_relevance", 0)
        if avg_relevance > 0:
            relevance_scores.append(avg_relevance)
    
    print(f"\n[ANALYSIS] ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„:")
    print(f"   - ì²­í¬ íƒ€ì… ë¶„í¬:")
    for chunk_type, count in chunk_type_summary.items():
        print(f"     * {chunk_type}: {count}ê°œ")
    
    if relevance_scores:
        avg_relevance = sum(relevance_scores) / len(relevance_scores)
        print(f"   - í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: {avg_relevance:.2f}")
    
    print(f"\n[OK] 1ë‹¨ê³„ í‰ê°€ ì™„ë£Œ!")
    print(f"ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ìˆ˜ë™ ë‹µë³€ ë°ì´í„°ë¥¼ ì´ìš©í•œ RAGAS êµ¬ì¡° ê²€ì¦ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print_section_header("RAG í‰ê°€ 1ë‹¨ê³„: Milvus ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€")
    print("í˜„ì¬ ì‘ë™í•˜ëŠ” Milvus ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì´ìš©í•œ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # 1. í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    questions = load_evaluation_dataset()
    if not questions:
        print("[FAIL] í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ì„ì‹œ)
    query_embeddings = create_simple_query_embeddings(questions)
    if not query_embeddings:
        print("[FAIL] ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
        return
    
    # 3. Milvus ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
    evaluation_results = evaluate_milvus_retrieval(query_embeddings)
    if not evaluation_results:
        print("[FAIL] ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨")
        return
    
    # 4. ê²°ê³¼ ì €ì¥ ë° ìš”ì•½
    save_evaluation_results(evaluation_results)
    print_evaluation_summary(evaluation_results)
    
    print(f"\n[NEXT] ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´:")
    print(f"   - 2ë‹¨ê³„: ìˆ˜ë™ ë‹µë³€ ë°ì´í„°ë¡œ RAGAS êµ¬ì¡° ê²€ì¦")
    print(f"   - 3ë‹¨ê³„: ê²€ìƒ‰ ê¸°ë°˜ í‰ê°€ ë©”íŠ¸ë¦­ ê°œë°œ")

if __name__ == "__main__":
    main()f"     [{j+1}] Score: {score:.4f}, Type: {chunk_type}")
                        print(f"         Text: {text}...")
                
                else:
                    print(f"   âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    
            except Exception as search_error:
                print(f"   âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {search_error}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        evaluation_results["performance_metrics"] = {
            "success_rate": successful_searches / len(query_embeddings),
            "average_search_time": total_search_time / len(query_embeddings),
            "total_search_time": total_search_time,
            "successful_searches": successful_searches,
            "failed_searches": len(query_embeddings) - successful_searches
        }
        
        client.close()
        print(f"\nâœ… ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ")
        return evaluation_results
        
    except Exception as e:
        print(f"âŒ Milvus ê²€ìƒ‰ í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {}

def analyze_search_results(results: List[Dict], question: str) -> Dict[str, Any]:
    """ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„"""
    if not results:
        return {"relevance_indicators": [], "chunk_type_distribution": {}, "score_statistics": {}}
    
    # ê´€ë ¨ì„± ì§€í‘œ ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)
    question_keywords = extract_keywords(question)
    relevance_indicators = []
    
    for result in results:
        text = result.get("text", "").lower()
        relevance_score = sum(1 for keyword in question_keywords if keyword.lower() in text)
        relevance_indicators.append(relevance_score)
    
    # ì²­í¬ íƒ€ì… ë¶„í¬
    chunk_types = {}
    for result in results:
        chunk_type = result.get("chunk_type", "unknown")
        chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
    
    # ì ìˆ˜ í†µê³„
    scores = [result.get("score", 0) for result in results]
    score_stats = {
        "min_score": min(scores) if scores else 0,
        "max_score": max(scores) if scores else 0,
        "avg_score": sum(scores) / len(scores) if scores else 0,
        "score_range": max(scores) - min(scores) if scores else 0
    }
    
    return {
        "relevance_indicators": relevance_indicators,
        "chunk_type_distribution": chunk_types,
        "score_statistics": score_stats,
        "avg_relevance": sum(relevance_indicators) / len(relevance_indicators) if relevance_indicators else 0
    }

def extract_keywords(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)"""
    # ë³´í—˜ ê´€ë ¨ ì£¼ìš” í‚¤ì›Œë“œ
    insurance_keywords = [
        'ë³´í—˜', 'ê³„ì•½', 'ë³´ì¥', 'ì§€ê¸‰', 'ì²­êµ¬', 'ë³´í—˜ê¸ˆ', 'ë³´í—˜ë£Œ', 'í”¼ë³´í—˜ì', 'ê³„ì•½ì',
        'íŠ¹ì•½', 'í•´ì§€', 'í•´ì•½', 'ë‚©ì…', 'ê¸‰ì—¬ê¸ˆ', 'ìˆ˜ìµì', 'ì§„ë‹¨', 'ì§ˆë³‘', 'ìƒí•´', 
        'ì¬í•´', 'ì¥í•´', 'ì‚¬ë§', 'ë§Œê¸°', 'ê°±ì‹ ', 'ë¶€í™œ', 'ë©´ì œ'
    ]
    
    found_keywords = []
    text_lower = text.lower()
    
    for keyword in insurance_keywords:
        if keyword in text_lower:
            found_keywords.append(keyword)
    
    return found_keywords

def save_evaluation_results(results: Dict[str, Any], filename: str = None):
    """í‰ê°€ ê²°ê³¼ ì €ì¥"""
    if not results:
        print("âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"evaluation_results/step1_milvus_retrieval_{timestamp}.json"
    
    try:
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… í‰ê°€ ê²°ê³¼ ì €ì¥: {filename}")
        
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

def print_evaluation_summary(results: Dict[str, Any]):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    if not results:
        return
    
    print_section_header("í‰ê°€ ê²°ê³¼ ìš”ì•½")
    
    metrics = results.get("performance_metrics", {})
    
    print(f"ğŸ“Š ê²€ìƒ‰ ì„±ëŠ¥ í†µê³„:")
    print(f"   - ì´ ì¿¼ë¦¬ ìˆ˜: {results.get('total_queries', 0)}ê°œ")
    print(f"   - ì„±ê³µë¥ : {metrics.get('success_rate', 0):.1%}")
    print(f"   - í‰ê·  ê²€ìƒ‰ ì‹œê°„: {metrics.get('average_search_time', 0):.3f}ì´ˆ")
    print(f"   - ì„±ê³µí•œ ê²€ìƒ‰: {metrics.get('successful_searches', 0)}ê°œ")
    print(f"   - ì‹¤íŒ¨í•œ ê²€ìƒ‰: {metrics.get('failed_searches', 0)}ê°œ")
    
    # ì²­í¬ íƒ€ì… ë¶„ì„
    chunk_type_summary = {}
    relevance_scores = []
    
    for result in results.get("search_results", []):
        analysis = result.get("analysis", {})
        
        # ì²­í¬ íƒ€ì… ì§‘ê³„
        for chunk_type, count in analysis.get("chunk_type_distribution", {}).items():
            chunk_type_summary[chunk_type] = chunk_type_summary.get(chunk_type, 0) + count
        
        # ê´€ë ¨ì„± ì ìˆ˜ ìˆ˜ì§‘
        avg_relevance = analysis.get("avg_relevance", 0)
        if avg_relevance > 0:
            relevance_scores.append(avg_relevance)
    
    print(f"\nğŸ“ˆ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„:")
    print(f"   - ì²­í¬ íƒ€ì… ë¶„í¬:")
    for chunk_type, count in chunk_type_summary.items():
        print(f"     * {chunk_type}: {count}ê°œ")
    
    if relevance_scores:
        avg_relevance = sum(relevance_scores) / len(relevance_scores)
        print(f"   - í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: {avg_relevance:.2f}")
    
    print(f"\nâœ… 1ë‹¨ê³„ í‰ê°€ ì™„ë£Œ!")
    print(f"ë‹¤ìŒ ë‹¨ê³„ì—ì„œëŠ” ìˆ˜ë™ ë‹µë³€ ë°ì´í„°ë¥¼ ì´ìš©í•œ RAGAS êµ¬ì¡° ê²€ì¦ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print_section_header("RAG í‰ê°€ 1ë‹¨ê³„: Milvus ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€")
    print("í˜„ì¬ ì‘ë™í•˜ëŠ” Milvus ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì´ìš©í•œ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # 1. í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    questions = load_evaluation_dataset()
    if not questions:
        print("âŒ í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ì„ì‹œ)
    query_embeddings = create_simple_query_embeddings(questions)
    if not query_embeddings:
        print("âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
        return
    
    # 3. Milvus ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
    evaluation_results = evaluate_milvus_retrieval(query_embeddings)
    if not evaluation_results:
        print("âŒ ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨")
        return
    
    # 4. ê²°ê³¼ ì €ì¥ ë° ìš”ì•½
    save_evaluation_results(evaluation_results)
    print_evaluation_summary(evaluation_results)
    
    print(f"\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´:")
    print(f"   - 2ë‹¨ê³„: ìˆ˜ë™ ë‹µë³€ ë°ì´í„°ë¡œ RAGAS êµ¬ì¡° ê²€ì¦")
    print(f"   - 3ë‹¨ê³„: ê²€ìƒ‰ ê¸°ë°˜ í‰ê°€ ë©”íŠ¸ë¦­ ê°œë°œ")

if __name__ == "__main__":
    main()

#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Step4 ê°œì„ ëœ RAGAS í‰ê°€ê¸° - Answer Relevancy & Context Relevancy í–¥ìƒ
"""

import os
import sys
import json
import logging
import traceback
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.evaluation.simple_ragas_metrics import SimpleRAGASMetrics
from src.evaluation.improved_answer_generator import ImprovedAnswerGenerator, ImprovedContextSearcher


class Step4ImprovedRAGASEvaluator:
    """4ë‹¨ê³„ ê°œì„ ëœ RAGAS í‰ê°€ê¸°"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.logger = self._setup_logging()
        self.project_root = project_root
        
        # ê°œì„ ëœ êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™”
        try:
            from src.evaluation.step3_standard_ragas_evaluator import AmnestyDataLoader, AmnestyMilvusSearcher
            
            self.data_loader = AmnestyDataLoader()
            self.searcher = AmnestyMilvusSearcher()
            self.answer_generator = ImprovedAnswerGenerator()  # ê°œì„ ëœ ë‹µë³€ ìƒì„±ê¸°
            self.context_searcher = ImprovedContextSearcher()  # ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ê¸°
            self.metrics = SimpleRAGASMetrics()
            
            self.logger.info("ê°œì„ ëœ êµ¬ì„±ìš”ì†Œê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
            
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.results_dir = self.project_root / "evaluation_results" / "step4_improved_ragas"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥ìš©
        self.evaluation_results = []
        self.overall_metrics = {}
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger("Step4ImprovedRAGASEvaluator")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            
            log_dir = project_root / "logs"
            log_dir.mkdir(exist_ok=True)
            file_handler = logging.FileHandler(log_dir / "step4_improved_ragas_evaluation.log", encoding='utf-8')
            file_handler.setLevel(logging.DEBUG)
            
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(formatter)
            file_handler.setFormatter(formatter)
            
            logger.addHandler(console_handler)
            logger.addHandler(file_handler)
            
        return logger
    
    def run_evaluation(self) -> Dict[str, Any]:
        """ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        self.logger.info("=== 4ë‹¨ê³„ ê°œì„ ëœ RAGAS í‰ê°€ ì‹œì‘ ===")
        start_time = datetime.now()
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            self.logger.info("Amnesty QA í‰ê°€ ë°ì´í„° ë¡œë”© ì¤‘...")
            evaluation_items = self.data_loader.load_evaluation_dataset()
            
            if not evaluation_items:
                self.logger.error("ë¡œë“œëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return {'success': False, 'error': 'ë¡œë“œëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.', 'processed_count': 0}
            
            self.logger.info(f"ë¡œë“œëœ ì§ˆë¬¸ ìˆ˜: {len(evaluation_items)}")
            
            # 2. ê°œì„ ëœ í‰ê°€ ì‹¤í–‰
            total_questions = len(evaluation_items)
            processed_count = 0
            
            for item in evaluation_items:
                try:
                    result = self._evaluate_single_item_improved(item)
                    self.evaluation_results.append(result)
                    processed_count += 1
                    
                    if processed_count % 2 == 0:
                        self.logger.info(f"ì§„í–‰ë¥ : {processed_count}/{total_questions} ({processed_count/total_questions*100:.1f}%)")
                        
                except Exception as e:
                    self.logger.error(f"ì§ˆë¬¸ '{item.get('question', 'Unknown')}' í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            # 3. ì „ì²´ í‰ê°€ ì§€í‘œ ê³„ì‚°
            self._calculate_overall_metrics()
            
            # 4. ê²°ê³¼ ì €ì¥
            result_file = self.save_results()
            
            # 5. ê°œì„  ë¦¬í¬íŠ¸ ìƒì„±
            self.generate_improvement_report()
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            self.logger.info(f"=== 4ë‹¨ê³„ ê°œì„ ëœ RAGAS í‰ê°€ ì™„ë£Œ ===")
            self.logger.info(f"ì´ ì†Œìš” ì‹œê°„: {duration.total_seconds():.2f}ì´ˆ")
            self.logger.info(f"ì²˜ë¦¬ëœ ì§ˆë¬¸ ìˆ˜: {processed_count}/{total_questions}")
            self.logger.info(f"ê²°ê³¼ ì €ì¥: {result_file}")
            
            return {
                'success': True,
                'processed_count': processed_count,
                'total_count': total_questions,
                'duration_seconds': duration.total_seconds(),
                'result_file': str(result_file),
                'overall_metrics': self.overall_metrics
            }
            
        except Exception as e:
            self.logger.error(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.logger.error(traceback.format_exc())
            return {'success': False, 'error': str(e), 'processed_count': len(self.evaluation_results)}
    
    def _evaluate_single_item_improved(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œì„ ëœ ê°œë³„ ì§ˆë¬¸ í‰ê°€"""
        question = item.get('question', '')
        ground_truths = item.get('ground_truths', [])
        original_contexts = item.get('contexts', [])
        
        # 1. ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        try:
            retrieved_contexts = self.searcher.search_contexts(question, top_k=5)
            all_contexts = original_contexts + retrieved_contexts
            
            # ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ ì„ ë³„
            improved_contexts = self.context_searcher.improved_context_search(
                question, all_contexts, top_k=3
            )
            
        except Exception as e:
            self.logger.warning(f"ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            improved_contexts = original_contexts
        
        # 2. ê°œì„ ëœ ë‹µë³€ ìƒì„±
        try:
            generated_answer = self.answer_generator.generate_improved_answer(
                question, improved_contexts
            )
        except Exception as e:
            self.logger.warning(f"ê°œì„ ëœ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê¸°ë³¸ ë‹µë³€ ìƒì„±
            generated_answer = "Unable to generate improved answer due to processing error."
        
        # 3. RAGAS ì§€í‘œ í‰ê°€
        try:
            ragas_result = self.metrics.evaluate_single_item(
                question_id=item.get('id', 'unknown'),
                question=question,
                contexts=improved_contexts,
                answer=generated_answer,
                ground_truth_contexts=original_contexts
            )
            
            metrics_dict = {
                'context_relevancy': ragas_result.context_relevancy,
                'context_precision': ragas_result.context_precision,
                'context_recall': ragas_result.context_recall,
                'faithfulness': ragas_result.faithfulness,
                'answer_relevancy': ragas_result.answer_relevancy,
                'overall_score': (ragas_result.context_relevancy + 
                                ragas_result.context_precision + 
                                ragas_result.context_recall + 
                                ragas_result.faithfulness + 
                                ragas_result.answer_relevancy) / 5.0
            }
            
        except Exception as e:
            self.logger.error(f"RAGAS ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            metrics_dict = {
                'context_relevancy': 0.0,
                'context_precision': 0.0,
                'context_recall': 0.0,
                'faithfulness': 0.0,
                'answer_relevancy': 0.0,
                'overall_score': 0.0,
                'error': str(e)
            }
        
        # 4. ê²°ê³¼ êµ¬ì„±
        result = {
            'question_id': item.get('id', 'unknown'),
            'question': question,
            'generated_answer': generated_answer,
            'ground_truths': ground_truths,
            'contexts': improved_contexts,
            'context_count': len(improved_contexts),
            'metrics': metrics_dict,
            'improvements': {
                'used_improved_context_search': True,
                'used_improved_answer_generation': True,
                'original_context_count': len(original_contexts),
                'improved_context_count': len(improved_contexts)
            },
            'metadata': item.get('metadata', {}),
            'evaluation_timestamp': datetime.now().isoformat()
        }
        
        return result
    
    def _calculate_overall_metrics(self) -> None:
        """ì „ì²´ í‰ê°€ ì§€í‘œ ê³„ì‚°"""
        if not self.evaluation_results:
            self.overall_metrics = {'error': 'í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.'}
            return
        
        # ê° ì§€í‘œë³„ ì ìˆ˜ ìˆ˜ì§‘
        metric_names = ['context_relevancy', 'context_precision', 'context_recall', 'faithfulness', 'answer_relevancy']
        metric_scores = {name: [] for name in metric_names}
        overall_scores = []
        
        for result in self.evaluation_results:
            metrics = result.get('metrics', {})
            for name in metric_names:
                score = metrics.get(name, 0.0)
                metric_scores[name].append(score)
            
            overall_scores.append(metrics.get('overall_score', 0.0))
        
        # í†µê³„ ê³„ì‚°
        self.overall_metrics = {
            'total_questions': len(self.evaluation_results),
            'evaluation_type': 'improved_ragas_5_metrics'
        }
        
        # ê° ì§€í‘œë³„ í†µê³„
        for name in metric_names:
            scores = metric_scores[name]
            self.overall_metrics[f'average_{name}'] = sum(scores) / len(scores) if scores else 0.0
            self.overall_metrics[f'min_{name}'] = min(scores) if scores else 0.0
            self.overall_metrics[f'max_{name}'] = max(scores) if scores else 0.0
        
        # ì „ì²´ ì ìˆ˜ í†µê³„
        self.overall_metrics['average_overall_score'] = sum(overall_scores) / len(overall_scores) if overall_scores else 0.0
        self.overall_metrics['min_overall_score'] = min(overall_scores) if overall_scores else 0.0
        self.overall_metrics['max_overall_score'] = max(overall_scores) if overall_scores else 0.0
    
    def save_results(self) -> Path:
        """í‰ê°€ ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = self.results_dir / f"step4_improved_ragas_evaluation_{timestamp}.json"
        
        # ì „ì²´ ê²°ê³¼ êµ¬ì„±
        full_results = {
            'evaluation_info': {
                'type': 'step4_improved_ragas_evaluation',
                'dataset': 'amnesty_qa',
                'improvements': [
                    'improved_context_search_with_entity_matching',
                    'improved_answer_generation_with_question_type_analysis',
                    'removed_unnecessary_prefixes',
                    'enhanced_relevancy_scoring'
                ],
                'metrics': ['context_relevancy', 'context_precision', 'context_recall', 'faithfulness', 'answer_relevancy'],
                'timestamp': datetime.now().isoformat(),
                'total_questions': len(self.evaluation_results),
                'evaluator_version': '2.0.0'
            },
            'overall_metrics': self.overall_metrics,
            'detailed_results': self.evaluation_results
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(full_results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ê°œì„ ëœ í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {result_file}")
        return result_file
    
    def generate_improvement_report(self) -> None:
        """ê°œì„  ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.overall_metrics:
            self.logger.warning("ì „ì²´ ì§€í‘œê°€ ê³„ì‚°ë˜ì§€ ì•Šì•„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*80)
        print("    ğŸš€ 4ë‹¨ê³„ ê°œì„ ëœ RAGAS í‰ê°€ ê²°ê³¼ (Amnesty QA)")
        print("="*80)
        
        print(f"í‰ê°€ ëŒ€ìƒ ì§ˆë¬¸ ìˆ˜: {self.overall_metrics.get('total_questions', 0)}ê°œ")
        print(f"âœ¨ ê°œì„ ëœ ì „ì²´ í‰ê·  ì ìˆ˜: {self.overall_metrics.get('average_overall_score', 0):.3f}")
        
        print("\n[ ğŸ¯ ê°œì„ ëœ RAGAS 5ê°œ ì§€í‘œ ]")
        metric_labels = {
            'context_relevancy': 'ğŸ” Context Relevancy (ê°œì„  ëª©í‘œ)',
            'context_precision': 'ğŸ¯ Context Precision', 
            'context_recall': 'ğŸ“š Context Recall',
            'faithfulness': 'ğŸ’¯ Faithfulness',
            'answer_relevancy': 'ğŸ’¬ Answer Relevancy (ê°œì„  ëª©í‘œ)'
        }
        
        for metric_key, label in metric_labels.items():
            avg = self.overall_metrics.get(f'average_{metric_key}', 0)
            min_val = self.overall_metrics.get(f'min_{metric_key}', 0)
            max_val = self.overall_metrics.get(f'max_{metric_key}', 0)
            
            # ê°œì„  ëª©í‘œ ì§€í‘œ í‘œì‹œ
            improvement_indicator = ""
            if metric_key in ['context_relevancy', 'answer_relevancy']:
                if avg > 0.6:
                    improvement_indicator = " ğŸŸ¢ ëª©í‘œ ë‹¬ì„±!"
                elif avg > 0.4:
                    improvement_indicator = " ğŸŸ¡ ê°œì„  ì¤‘"
                else:
                    improvement_indicator = " ğŸ”´ ì¶”ê°€ ê°œì„  í•„ìš”"
            
            print(f"{label:30}: {avg:.3f} (ìµœì†Œ: {min_val:.3f}, ìµœëŒ€: {max_val:.3f}){improvement_indicator}")
        
        # ì„±ëŠ¥ í‰ê°€
        overall_score = self.overall_metrics.get('average_overall_score', 0)
        if overall_score >= 0.7:
            performance = "ğŸŒŸ ìš°ìˆ˜í•¨ (Excellent)"
        elif overall_score >= 0.5:
            performance = "âœ… ì–‘í˜¸í•¨ (Good)"
        elif overall_score >= 0.3:
            performance = "âš ï¸ ë³´í†µ (Fair)"
        else:
            performance = "âŒ ê°œì„  í•„ìš” (Poor)"
        
        print(f"\nğŸ† ì „ì²´ ì„±ëŠ¥ í‰ê°€: {performance}")
        print(f"ğŸ“Š ë°ì´í„°ì…‹: Amnesty QA (Human Rights)")
        print(f"ğŸ”§ í‰ê°€ ë°©ì‹: ê°œì„ ëœ RAGAS 5ê°œ ì§€í‘œ")
        print(f"ğŸš€ ì ìš©ëœ ê°œì„ ì‚¬í•­:")
        print(f"   - ì§ˆë¬¸ ìœ í˜•ë³„ ë§ì¶¤ ë‹µë³€ ìƒì„±")
        print(f"   - ì—”í‹°í‹° ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰")
        print(f"   - ë¶ˆí•„ìš”í•œ ì ‘ë‘ì‚¬ ì œê±°")
        print(f"   - ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°")
        print("="*80 + "\n")


# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    try:
        evaluator = Step4ImprovedRAGASEvaluator()
        result = evaluator.run_evaluation()
        
        if result['success']:
            print(f"\nğŸ‰ ê°œì„ ëœ í‰ê°€ ì™„ë£Œ! ì²˜ë¦¬ëœ ì§ˆë¬¸: {result['processed_count']}/{result['total_count']}")
            print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {result['result_file']}")
        else:
            print(f"\nâŒ í‰ê°€ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
End-to-End RAG íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸
PDF íŒŒì‹±ë¶€í„° Re-rankingê¹Œì§€ ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì˜ ì™„ì „í•œ íë¦„ì„ ê²€ì¦í•©ë‹ˆë‹¤.
ì‹¤ì œ amnesty_qa ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ê° ë‹¨ê³„ë³„ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ì™€ ì†Œìš” ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import psutil
import logging
import traceback
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ.setdefault('LOG_LEVEL', 'INFO')

try:
    from src.utils.config import Config
    from src.rag.retriever import DocumentRetriever
    from src.rag.embedder import DocumentEmbedder
    from src.parsers.pdf_parser import PDFParser
    from src.vectordb.milvus_client import MilvusClient
    from src.utils.logger import get_logger
    from src.evaluation.simple_ragas_metrics import SimpleRAGASMetrics
except ImportError as e:
    print(f"Import error: {e}")
    print("Please ensure you're running from the project root directory")
    sys.exit(1)

# ë¡œê·¸ ì„¤ì •
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

logger = get_logger("end_to_end_pipeline", 
                   log_file=log_dir / f"end_to_end_test_{timestamp}.log")


class EndToEndPipelineTest:
    """End-to-End RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì´ˆê¸°í™”"""
        self.config = Config()
        self.test_results = {
            'test_start_time': datetime.now().isoformat(),
            'pipeline_stages': {},
            'performance_metrics': {},
            'memory_usage': {},
            'test_queries': [],
            'overall_success': False,
            'error_log': []
        }
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.process = psutil.Process()
        self.initial_memory = self.get_memory_usage()
        
        logger.info("=== End-to-End RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        logger.info(f"ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {self.initial_memory:.2f} MB")
    
    def get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB ë‹¨ìœ„)"""
        return self.process.memory_info().rss / 1024 / 1024
    
    def log_stage_performance(self, stage_name: str, start_time: float, 
                            success: bool, error_msg: str = None) -> Dict[str, Any]:
        """ë‹¨ê³„ë³„ ì„±ëŠ¥ ë° ê²°ê³¼ ë¡œê¹…"""
        end_time = time.time()
        duration = end_time - start_time
        memory_usage = self.get_memory_usage()
        
        stage_result = {
            'success': success,
            'duration_seconds': round(duration, 3),
            'memory_mb': round(memory_usage, 2),
            'timestamp': datetime.now().isoformat()
        }
        
        if error_msg:
            stage_result['error'] = error_msg
            self.test_results['error_log'].append({
                'stage': stage_name,
                'error': error_msg,
                'timestamp': datetime.now().isoformat()
            })
        
        self.test_results['pipeline_stages'][stage_name] = stage_result
        
        status = "âœ“" if success else "âœ—"
        logger.info(f"{status} {stage_name}: {duration:.3f}ì´ˆ, ë©”ëª¨ë¦¬: {memory_usage:.2f}MB")
        
        if error_msg:
            logger.error(f"  ì˜¤ë¥˜: {error_msg}")
        
        return stage_result
    
    def test_01_system_initialization(self) -> bool:
        """1ë‹¨ê³„: ì‹œìŠ¤í…œ êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        logger.info("\n--- 1ë‹¨ê³„: ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ---")
        start_time = time.time()
        
        try:
            # Config ì´ˆê¸°í™”
            logger.info("Config ì´ˆê¸°í™” ì¤‘...")
            assert self.config is not None
            
            # MilvusClient ì—°ê²° í…ŒìŠ¤íŠ¸
            logger.info("Milvus ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            milvus_client = MilvusClient(self.config)
            milvus_connected = milvus_client.connect()
            
            if not milvus_connected:
                raise Exception("Milvus ì—°ê²° ì‹¤íŒ¨")
            
            # DocumentRetriever ì´ˆê¸°í™”
            logger.info("DocumentRetriever ì´ˆê¸°í™” ì¤‘...")
            self.retriever = DocumentRetriever(self.config)
            assert self.retriever is not None
            
            # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
            collections = milvus_client.list_collections()
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜: {len(collections)}ê°œ")
            
            if not collections:
                logger.warning("ê²½ê³ : ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            self.log_stage_performance("system_initialization", start_time, True)
            return True
            
        except Exception as e:
            error_msg = f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}"
            self.log_stage_performance("system_initialization", start_time, False, error_msg)
            return False
    
    def test_02_data_loading(self) -> bool:
        """2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ë° ê²€ì¦"""
        logger.info("\n--- 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ---")
        start_time = time.time()
        
        try:
            # amnesty_qa í‰ê°€ ë°ì´í„° ë¡œë”©
            data_path = Path("data/amnesty_qa/amnesty_qa_evaluation.json")
            
            if not data_path.exists():
                raise FileNotFoundError(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_path}")
            
            logger.info(f"ë°ì´í„° íŒŒì¼ ë¡œë”©: {data_path}")
            with open(data_path, 'r', encoding='utf-8') as f:
                self.test_data = json.load(f)
            
            # ë°ì´í„° êµ¬ì¡° ê²€ì¦
            if not isinstance(self.test_data, list):
                raise ValueError("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            
            if len(self.test_data) == 0:
                raise ValueError("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
            # ì²« ë²ˆì§¸ í•­ëª© êµ¬ì¡° ê²€ì¦
            first_item = self.test_data[0]
            required_fields = ['question', 'contexts', 'answer']
            
            for field in required_fields:
                if field not in first_item:
                    raise KeyError(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
            
            logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.test_data)}ê°œ í•­ëª©")
            
            # ìƒ˜í”Œ ë°ì´í„° ë¡œê·¸
            logger.info(f"ì²« ë²ˆì§¸ ì§ˆë¬¸ ì˜ˆì‹œ: {first_item['question'][:100]}...")
            
            self.log_stage_performance("data_loading", start_time, True)
            return True
            
        except Exception as e:
            error_msg = f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}"
            self.log_stage_performance("data_loading", start_time, False, error_msg)
            return False
    
    def test_03_vector_search_pipeline(self) -> bool:
        """3ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        logger.info("\n--- 3ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ---")
        start_time = time.time()
        
        try:
            # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 5ê°œ ì„ íƒ (ë°ì´í„°ì—ì„œ ì²˜ìŒ 5ê°œ)
            test_queries = []
            for i, item in enumerate(self.test_data[:5]):
                test_queries.append({
                    'id': i + 1,
                    'question': item['question'],
                    'expected_contexts': item.get('contexts', []),
                    'expected_answer': item.get('answer', '')
                })
            
            logger.info(f"{len(test_queries)}ê°œ ì§ˆë¬¸ìœ¼ë¡œ ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
            
            successful_queries = 0
            total_search_time = 0
            
            for query_info in test_queries:
                query_start = time.time()
                try:
                    # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
                    search_results = self.retriever.retrieve(
                        query=query_info['question'],
                        top_k=5,
                        force_filter_expr=None
                    )
                    
                    query_duration = time.time() - query_start
                    total_search_time += query_duration
                    
                    # ê²°ê³¼ ê²€ì¦
                    if isinstance(search_results, list) and len(search_results) > 0:
                        successful_queries += 1
                        
                        # ê²°ê³¼ ì •ë³´ ë¡œê¹…
                        logger.info(f"  ì§ˆë¬¸ {query_info['id']}: {len(search_results)}ê°œ ê²°ê³¼, {query_duration:.3f}ì´ˆ")
                        
                        # ìƒìœ„ ê²°ê³¼ ìƒ˜í”Œ ë¡œê¹…
                        top_result = search_results[0]
                        similarity = top_result.get('similarity', top_result.get('score', 0))
                        logger.info(f"    ìµœê³  ìœ ì‚¬ë„: {similarity:.4f}")
                        
                        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì €ì¥
                        self.test_results['test_queries'].append({
                            'id': query_info['id'],
                            'question': query_info['question'][:100] + '...' if len(query_info['question']) > 100 else query_info['question'],
                            'results_count': len(search_results),
                            'search_duration': round(query_duration, 3),
                            'top_similarity': round(similarity, 4) if isinstance(similarity, (int, float)) else str(similarity)
                        })
                    else:
                        logger.warning(f"  ì§ˆë¬¸ {query_info['id']}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                        
                except Exception as e:
                    logger.error(f"  ì§ˆë¬¸ {query_info['id']} ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                    query_duration = time.time() - query_start
                    total_search_time += query_duration
            
            # ì „ì²´ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            success_rate = (successful_queries / len(test_queries)) * 100
            avg_search_time = total_search_time / len(test_queries)
            
            logger.info(f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {successful_queries}/{len(test_queries)} ì„±ê³µ ({success_rate:.1f}%)")
            logger.info(f"í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_search_time:.3f}ì´ˆ")
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
            self.test_results['performance_metrics']['vector_search'] = {
                'success_rate': round(success_rate, 2),
                'avg_search_time': round(avg_search_time, 3),
                'total_queries': len(test_queries),
                'successful_queries': successful_queries
            }
            
            # ì„±ê³µ ê¸°ì¤€: 95% ì´ìƒ ì„±ê³µë¥ 
            pipeline_success = success_rate >= 95.0
            
            if pipeline_success:
                self.log_stage_performance("vector_search_pipeline", start_time, True)
            else:
                error_msg = f"ë²¡í„° ê²€ìƒ‰ ì„±ê³µë¥  ë¶€ì¡±: {success_rate:.1f}% (95% ë¯¸ë§Œ)"
                self.log_stage_performance("vector_search_pipeline", start_time, False, error_msg)
            
            return pipeline_success
            
        except Exception as e:
            error_msg = f"ë²¡í„° ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}"
            self.log_stage_performance("vector_search_pipeline", start_time, False, error_msg)
            return False
    
    def test_04_hybrid_search_with_reranking(self) -> bool:
        """4ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° Re-ranking í…ŒìŠ¤íŠ¸"""
        logger.info("\n--- 4ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + Re-ranking í…ŒìŠ¤íŠ¸ ---")
        start_time = time.time()
        
        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° Re-ranking í™œì„±í™” í™•ì¸
            has_hybrid = hasattr(self.retriever, 'hybrid_retrieve')
            has_reranker = hasattr(self.retriever, 'reranker') and self.retriever.reranker is not None
            
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì›: {has_hybrid}")
            logger.info(f"Re-ranking ì§€ì›: {has_reranker}")
            
            # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 3ê°œ ì„ íƒ
            test_queries = self.test_data[:3]
            successful_hybrid = 0
            total_hybrid_time = 0
            
            for i, item in enumerate(test_queries):
                query_start = time.time()
                try:
                    query = item['question']
                    
                    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ (ê°€ëŠ¥í•œ ê²½ìš°)
                    if has_hybrid:
                        hybrid_results = self.retriever.hybrid_retrieve(
                            query=query,
                            top_k=5,
                            use_parent_chunks=False
                        )
                    else:
                        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰ ì‚¬ìš©
                        hybrid_results = self.retriever.retrieve(
                            query=query,
                            top_k=5
                        )
                    
                    query_duration = time.time() - query_start
                    total_hybrid_time += query_duration
                    
                    # ê²°ê³¼ ê²€ì¦
                    if isinstance(hybrid_results, list) and len(hybrid_results) > 0:
                        successful_hybrid += 1
                        
                        # Re-ranking ì ìˆ˜ í™•ì¸
                        has_rerank_scores = any('rerank_score' in result for result in hybrid_results)
                        has_hybrid_scores = any('hybrid_score' in result for result in hybrid_results)
                        
                        logger.info(f"  ì§ˆë¬¸ {i+1}: {len(hybrid_results)}ê°œ ê²°ê³¼, {query_duration:.3f}ì´ˆ")
                        logger.info(f"    Hybrid ì ìˆ˜ í¬í•¨: {has_hybrid_scores}")
                        logger.info(f"    Re-rank ì ìˆ˜ í¬í•¨: {has_rerank_scores}")
                        
                        # ìƒìœ„ ê²°ê³¼ ìƒ˜í”Œ
                        top_result = hybrid_results[0]
                        similarity = top_result.get('similarity', 0)
                        hybrid_score = top_result.get('hybrid_score', 'N/A')
                        rerank_score = top_result.get('rerank_score', 'N/A')
                        
                        logger.info(f"    ìƒìœ„ ê²°ê³¼ - Similarity: {similarity:.4f}, Hybrid: {hybrid_score}, Rerank: {rerank_score}")
                        
                    else:
                        logger.warning(f"  ì§ˆë¬¸ {i+1}: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                        
                except Exception as e:
                    logger.error(f"  ì§ˆë¬¸ {i+1} í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                    query_duration = time.time() - query_start
                    total_hybrid_time += query_duration
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ëŠ¥ ê³„ì‚°
            hybrid_success_rate = (successful_hybrid / len(test_queries)) * 100
            avg_hybrid_time = total_hybrid_time / len(test_queries)
            
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼: {successful_hybrid}/{len(test_queries)} ì„±ê³µ ({hybrid_success_rate:.1f}%)")
            logger.info(f"í‰ê·  í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œê°„: {avg_hybrid_time:.3f}ì´ˆ")
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
            self.test_results['performance_metrics']['hybrid_search'] = {
                'success_rate': round(hybrid_success_rate, 2),
                'avg_search_time': round(avg_hybrid_time, 3),
                'total_queries': len(test_queries),
                'successful_queries': successful_hybrid,
                'has_hybrid_feature': has_hybrid,
                'has_reranking_feature': has_reranker
            }
            
            # ì„±ê³µ ê¸°ì¤€: 90% ì´ìƒ ì„±ê³µë¥ 
            pipeline_success = hybrid_success_rate >= 90.0
            
            if pipeline_success:
                self.log_stage_performance("hybrid_search_reranking", start_time, True)
            else:
                error_msg = f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ê³µë¥  ë¶€ì¡±: {hybrid_success_rate:.1f}% (90% ë¯¸ë§Œ)"
                self.log_stage_performance("hybrid_search_reranking", start_time, False, error_msg)
            
            return pipeline_success
            
        except Exception as e:
            error_msg = f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + Re-ranking ì‹¤íŒ¨: {str(e)}"
            self.log_stage_performance("hybrid_search_reranking", start_time, False, error_msg)
            return False
    
    def test_05_memory_performance_validation(self) -> bool:
        """5ë‹¨ê³„: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì„±ëŠ¥ ê²€ì¦"""
        logger.info("\n--- 5ë‹¨ê³„: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì„±ëŠ¥ ê²€ì¦ ---")
        start_time = time.time()
        
        try:
            current_memory = self.get_memory_usage()
            memory_increase = current_memory - self.initial_memory
            
            logger.info(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {current_memory:.2f} MB")
            logger.info(f"ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰: {memory_increase:.2f} MB")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ì¤€ í™•ì¸ (4GB = 4096 MB)
            memory_limit_mb = 4096
            memory_within_limit = current_memory <= memory_limit_mb
            
            # ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œê°„ ê³„ì‚°
            total_test_time = time.time() - self.test_results.get('test_start_time_float', time.time())
            
            # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
            performance_summary = {
                'total_test_duration': round(total_test_time, 3),
                'memory_usage': {
                    'initial_mb': round(self.initial_memory, 2),
                    'current_mb': round(current_memory, 2),
                    'increase_mb': round(memory_increase, 2),
                    'within_limit': memory_within_limit,
                    'limit_mb': memory_limit_mb
                }
            }
            
            # ê° ë‹¨ê³„ë³„ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            stages = self.test_results.get('pipeline_stages', {})
            successful_stages = sum(1 for stage in stages.values() if stage.get('success', False))
            total_stages = len(stages)
            
            logger.info(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {total_test_time:.3f}ì´ˆ")
            logger.info(f"ì„±ê³µí•œ ë‹¨ê³„: {successful_stages}/{total_stages}")
            logger.info(f"ë©”ëª¨ë¦¬ ì œí•œ ë‚´: {memory_within_limit} ({current_memory:.2f}MB / {memory_limit_mb}MB)")
            
            # ì „ì²´ ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
            performance_checks = {
                'total_time_under_10s': total_test_time <= 10.0,
                'memory_under_4gb': memory_within_limit,
                'stages_success_rate_95': (successful_stages / total_stages) >= 0.95 if total_stages > 0 else False
            }
            
            all_checks_passed = all(performance_checks.values())
            
            # ì„±ëŠ¥ ë° ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ì €ì¥
            self.test_results['performance_metrics']['overall'] = performance_summary
            self.test_results['performance_metrics']['validation_checks'] = performance_checks
            self.test_results['memory_usage']['final'] = {
                'memory_mb': round(current_memory, 2),
                'within_limit': memory_within_limit
            }
            
            # ì„±ëŠ¥ ê²€ì¦ ê²°ê³¼ ë¡œê¹…
            for check_name, passed in performance_checks.items():
                status = "âœ“" if passed else "âœ—"
                logger.info(f"  {status} {check_name}: {passed}")
            
            if all_checks_passed:
                self.log_stage_performance("memory_performance_validation", start_time, True)
            else:
                failed_checks = [name for name, passed in performance_checks.items() if not passed]
                error_msg = f"ì„±ëŠ¥ ê²€ì¦ ì‹¤íŒ¨: {', '.join(failed_checks)}"
                self.log_stage_performance("memory_performance_validation", start_time, False, error_msg)
            
            return all_checks_passed
            
        except Exception as e:
            error_msg = f"ë©”ëª¨ë¦¬ ì„±ëŠ¥ ê²€ì¦ ì‹¤íŒ¨: {str(e)}"
            self.log_stage_performance("memory_performance_validation", start_time, False, error_msg)
            return False
    
    def run_complete_pipeline_test(self) -> Dict[str, Any]:
        """End-to-End íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("íŒŒì´í”„ë¼ì¸ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„ ì €ì¥
        self.test_results['test_start_time_float'] = time.time()
        
        # ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_stages = [
            ('01_system_initialization', self.test_01_system_initialization),
            ('02_data_loading', self.test_02_data_loading),
            ('03_vector_search_pipeline', self.test_03_vector_search_pipeline),
            ('04_hybrid_search_reranking', self.test_04_hybrid_search_with_reranking),
            ('05_memory_performance_validation', self.test_05_memory_performance_validation)
        ]
        
        all_stages_successful = True
        
        for stage_name, test_function in test_stages:
            try:
                logger.info(f"\n{'='*60}")
                logger.info(f"ë‹¨ê³„ ì‹¤í–‰: {stage_name}")
                logger.info(f"{'='*60}")
                
                stage_success = test_function()
                
                if not stage_success:
                    all_stages_successful = False
                    logger.error(f"ë‹¨ê³„ ì‹¤íŒ¨: {stage_name}")
                else:
                    logger.info(f"ë‹¨ê³„ ì„±ê³µ: {stage_name}")
                
            except Exception as e:
                all_stages_successful = False
                logger.error(f"ë‹¨ê³„ ì˜ˆì™¸ ë°œìƒ {stage_name}: {str(e)}")
                logger.error(f"Stack trace: {traceback.format_exc()}")
        
        # ìµœì¢… ê²°ê³¼ ì •ë¦¬
        self.test_results['test_end_time'] = datetime.now().isoformat()
        self.test_results['overall_success'] = all_stages_successful
        
        total_duration = time.time() - self.test_results['test_start_time_float']
        self.test_results['total_duration_seconds'] = round(total_duration, 3)
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
        final_memory = self.get_memory_usage()
        self.test_results['memory_usage']['final_mb'] = round(final_memory, 2)
        
        return self.test_results
    
    def save_test_results(self) -> str:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            results_dir = Path("evaluation_results")
            results_dir.mkdir(exist_ok=True)
            
            results_file = results_dir / f"end_to_end_test_results_{timestamp}.json"
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_file}")
            return str(results_file)
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return ""
    
    def print_final_summary(self):
        """ìµœì¢… í…ŒìŠ¤íŠ¸ ìš”ì•½ ì¶œë ¥"""
        logger.info("\n" + "="*80)
        logger.info("END-TO-END RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìµœì¢… ìš”ì•½")
        logger.info("="*80)
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        overall_success = self.test_results.get('overall_success', False)
        total_duration = self.test_results.get('total_duration_seconds', 0)
        
        status_emoji = "âœ…" if overall_success else "âŒ"
        logger.info(f"{status_emoji} ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'ì„±ê³µ' if overall_success else 'ì‹¤íŒ¨'}")
        logger.info(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_duration:.3f}ì´ˆ")
        
        # ë‹¨ê³„ë³„ ê²°ê³¼ ìš”ì•½
        stages = self.test_results.get('pipeline_stages', {})
        logger.info(f"\nğŸ“‹ ë‹¨ê³„ë³„ ê²°ê³¼ ({len(stages)}ê°œ ë‹¨ê³„):")
        
        for stage_name, stage_info in stages.items():
            success = stage_info.get('success', False)
            duration = stage_info.get('duration_seconds', 0)
            memory = stage_info.get('memory_mb', 0)
            
            status = "âœ“" if success else "âœ—"
            logger.info(f"  {status} {stage_name}: {duration:.3f}ì´ˆ, {memory:.1f}MB")
            
            if not success and 'error' in stage_info:
                logger.info(f"    ì˜¤ë¥˜: {stage_info['error']}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìš”ì•½
        metrics = self.test_results.get('performance_metrics', {})
        logger.info(f"\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        
        if 'vector_search' in metrics:
            vs_metrics = metrics['vector_search']
            logger.info(f"  ë²¡í„° ê²€ìƒ‰: {vs_metrics.get('success_rate', 0):.1f}% ì„±ê³µ, {vs_metrics.get('avg_search_time', 0):.3f}ì´ˆ í‰ê· ")
        
        if 'hybrid_search' in metrics:
            hs_metrics = metrics['hybrid_search']
            logger.info(f"  í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: {hs_metrics.get('success_rate', 0):.1f}% ì„±ê³µ, {hs_metrics.get('avg_search_time', 0):.3f}ì´ˆ í‰ê· ")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìš”ì•½
        memory_info = self.test_results.get('memory_usage', {})
        final_memory = memory_info.get('final_mb', 0)
        logger.info(f"  ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {final_memory:.2f}MB")
        
        # ê¶Œì¥ì‚¬í•­
        logger.info(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        
        if not overall_success:
            logger.info("  - ì‹¤íŒ¨í•œ ë‹¨ê³„ì˜ ì˜¤ë¥˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”")
            logger.info("  - ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë° ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ ì ê²€í•˜ì„¸ìš”")
        
        if total_duration > 10:
            logger.info("  - ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‘ë‹µ ì‹œê°„ì´ ëª©í‘œ(10ì´ˆ)ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤")
            logger.info("  - ì„±ëŠ¥ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
        
        if final_memory > 3000:  # 3GB ì´ìƒ
            logger.info("  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸° ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”")
        
        logger.info("\nğŸ“ ìƒì„¸ ê²°ê³¼ëŠ” evaluation_results/ í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”")
        logger.info("="*80)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ End-to-End RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info(f"í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # í…ŒìŠ¤íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
        pipeline_test = EndToEndPipelineTest()
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results = pipeline_test.run_complete_pipeline_test()
        
        # ê²°ê³¼ ì €ì¥
        results_file = pipeline_test.save_test_results()
        
        # ìµœì¢… ìš”ì•½ ì¶œë ¥
        pipeline_test.print_final_summary()
        
        # ì¢…ë£Œ ìƒíƒœ ê²°ì •
        overall_success = test_results.get('overall_success', False)
        
        if overall_success:
            logger.info("âœ… End-to-End í…ŒìŠ¤íŠ¸ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
            return 0
        else:
            logger.error("âŒ End-to-End í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ìƒì„¸ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return 1
            
    except KeyboardInterrupt:
        logger.warning("âš ï¸ ì‚¬ìš©ìì— ì˜í•´ í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return 2
        
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"Stack trace: {traceback.format_exc()}")
        return 3


if __name__ == "__main__":
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # evaluation_results ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path("evaluation_results")
    results_dir.mkdir(exist_ok=True)
    
    # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
    exit_code = main()
    
    # ì¢…ë£Œ ì½”ë“œë¡œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    sys.exit(exit_code)

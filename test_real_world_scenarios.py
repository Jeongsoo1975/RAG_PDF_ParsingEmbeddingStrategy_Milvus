#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ RAG í†µí•© í…ŒìŠ¤íŠ¸
ë‹¤ì–‘í•œ ì§ˆë¬¸ ìœ í˜•, ë‹¤êµ­ì–´ ì²˜ë¦¬, ë¬¸ì„œ í¬ê¸° ë“±ì˜ ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
ì‹¤ì œ amnesty_qa ë°ì´í„°ì…‹ì˜ ë‹¤ì–‘ì„±ì„ í™œìš©í•˜ì—¬ 10ê°€ì§€ ì´ìƒì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import logging
import traceback
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ.setdefault('LOG_LEVEL', 'INFO')

try:
    from src.utils.config import Config
    from src.rag.retriever import DocumentRetriever
    from src.evaluation.simple_ragas_metrics import SimpleRAGASMetrics
    from src.utils.logger import setup_logger
except ImportError as e:
    print(f"Import error: {e}")
    print("Please ensure you're running from the project root directory")
    sys.exit(1)

# ë¡œê·¸ ì„¤ì •
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

logger = setup_logger("real_world_scenarios", 
                     log_file=log_dir / f"real_world_scenarios_{timestamp}.log")


@dataclass
class ScenarioResult:
    """ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    scenario_name: str
    success: bool
    accuracy_score: float
    context_relevancy: float
    answer_relevancy: float
    response_time: float
    error_message: Optional[str] = None
    additional_metrics: Optional[Dict[str, Any]] = None


class RealWorldScenarioTest:
    """ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ RAG í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì´ˆê¸°í™”"""
        self.config = Config()
        self.retriever = DocumentRetriever(self.config)
        self.metrics_evaluator = SimpleRAGASMetrics()
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        self.test_results = {
            'test_start_time': datetime.now().isoformat(),
            'scenario_results': [],
            'overall_summary': {},
            'performance_metrics': {},
            'test_data_info': {}
        }
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        self.load_test_data()
        
        logger.info("=== ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ RAG í†µí•© í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ ===")
    
    def calculate_context_relevancy(self, question: str, contexts: List[str]) -> float:
        """ë¬¸ë§¥ ê´€ë ¨ì„± ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        if not contexts:
            return 0.0
        
        # ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_lower = question.lower()
        question_keywords = set(word.strip('.,?!"()') for word in question_lower.split() if len(word) > 2)
        
        total_relevancy = 0
        for context in contexts:
            if not context:
                continue
                
            context_lower = context.lower()
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°
            matching_keywords = sum(1 for keyword in question_keywords if keyword in context_lower)
            relevancy = matching_keywords / len(question_keywords) if question_keywords else 0
            total_relevancy += relevancy
        
        return min(total_relevancy / len(contexts), 1.0)
    
    def calculate_answer_relevancy(self, question: str, expected_answer: str, contexts: List[str]) -> float:
        """ë‹µë³€ ê´€ë ¨ì„± ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        if not contexts or not expected_answer:
            return 0.0
        
        # ê¸°ëŒ€ ë‹µë³€ì˜ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
        answer_lower = expected_answer.lower()
        answer_keywords = set(word.strip('.,?!"()') for word in answer_lower.split() if len(word) > 3)
        
        # ë¬¸ë§¥ì—ì„œ ë‹µë³€ ë‚´ìš© ì°¾ê¸°
        total_coverage = 0
        for context in contexts:
            if not context:
                continue
                
            context_lower = context.lower()
            matching_keywords = sum(1 for keyword in answer_keywords if keyword in context_lower)
            coverage = matching_keywords / len(answer_keywords) if answer_keywords else 0
            total_coverage += coverage
        
        return min(total_coverage / len(contexts), 1.0)
    
    def calculate_accuracy_score(self, retrieved_contexts: List[str], expected_contexts: List[str]) -> float:
        """ì •í™•ë„ ì ìˆ˜ ê³„ì‚°"""
        if not retrieved_contexts or not expected_contexts:
            return 0.0
        
        # ê¸°ëŒ€ ë¬¸ë§¥ì˜ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ
        expected_text = ' '.join(expected_contexts).lower()
        expected_keywords = set(word.strip('.,?!"()') for word in expected_text.split() if len(word) > 3)
        
        # ê²€ìƒ‰ëœ ë¬¸ë§¥ì—ì„œ ë§¤ì¹­ í™•ì¸
        retrieved_text = ' '.join(retrieved_contexts).lower()
        matching_keywords = sum(1 for keyword in expected_keywords if keyword in retrieved_text)
        
        return matching_keywords / len(expected_keywords) if expected_keywords else 0.0
    
    def scenario_02_complex_analytical_questions(self) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ 2: ë³µì¡í•œ ë¶„ì„ì  ì§ˆë¬¸ ì²˜ë¦¬"""
        # ë¶„ì„ì´ í•„ìš”í•œ ì§ˆë¬¸ ì„ íƒ
        analytical_indices = [1, 7, 8]  # êµ­ì œë²• ì—­í• , ë„ì „ê³¼ì œ, ë¹ˆê³¤ê³¼ ì¸ê¶Œ
        
        total_accuracy = 0
        total_context_rel = 0
        total_answer_rel = 0
        successful_queries = 0
        
        for idx in analytical_indices:
            if idx < len(self.amnesty_data['questions']):
                question = self.amnesty_data['questions'][idx]
                expected_contexts = self.amnesty_data['contexts'][idx]
                expected_answer = self.amnesty_data['ground_truths'][idx][0]
                
                try:
                    # ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•´ ë” ë§ì€ ë¬¸ë§¥ ê²€ìƒ‰
                    search_results = self.retriever.retrieve(query=question, top_k=8)
                    
                    if search_results and len(search_results) > 0:
                        contexts = [result.get('content', result.get('text', '')) for result in search_results]
                        
                        context_relevancy = self.calculate_context_relevancy(question, contexts)
                        answer_relevancy = self.calculate_answer_relevancy(question, expected_answer, contexts)
                        accuracy = self.calculate_accuracy_score(contexts, expected_contexts)
                        
                        total_accuracy += accuracy
                        total_context_rel += context_relevancy
                        total_answer_rel += answer_relevancy
                        successful_queries += 1
                        
                        logger.info(f"  ë¶„ì„ ì§ˆë¬¸ {idx}: ì •í™•ë„={accuracy:.3f}")
                        
                except Exception as e:
                    logger.warning(f"  ë¶„ì„ ì§ˆë¬¸ {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        
        # í‰ê·  ê³„ì‚°
        if successful_queries > 0:
            avg_accuracy = total_accuracy / successful_queries
            avg_context_rel = total_context_rel / successful_queries
            avg_answer_rel = total_answer_rel / successful_queries
        else:
            avg_accuracy = avg_context_rel = avg_answer_rel = 0.0
        
        return {
            'accuracy_score': avg_accuracy,
            'context_relevancy': avg_context_rel,
            'answer_relevancy': avg_answer_rel,
            'additional_metrics': {
                'successful_queries': successful_queries,
                'total_queries': len(analytical_indices),
                'question_type': 'analytical',
                'top_k_used': 8
            }
        }
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        self.load_test_data()
        
        logger.info("=== ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ RAG í†µí•© í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ ===")
    
    def scenario_03_action_oriented_questions(self) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ 3: ì‹¤í–‰ ì§€í–¥ì  ì§ˆë¬¸ ì²˜ë¦¬"""
        # ì‹¤í–‰ ë°©ë²•ì„ ë¬»ëŠ” ì§ˆë¬¸ ì„ íƒ
        action_indices = [2, 5, 9]  # ì•°ë„¤ìŠ¤í‹° í™œë™, ê°œì¸ ê¸°ì—¬, ì¸ê¶Œ êµìœ¡
        
        total_accuracy = 0
        total_context_rel = 0
        total_answer_rel = 0
        successful_queries = 0
        
        for idx in action_indices:
            if idx < len(self.amnesty_data['questions']):
                question = self.amnesty_data['questions'][idx]
                expected_contexts = self.amnesty_data['contexts'][idx]
                expected_answer = self.amnesty_data['ground_truths'][idx][0]
                
                try:
                    # ì‹¤í–‰ ì¤‘ì‹¬ ì§ˆë¬¸ì„ ìœ„í•œ ê²€ìƒ‰
                    search_results = self.retriever.retrieve(query=question, top_k=6)
                    
                    if search_results and len(search_results) > 0:
                        contexts = [result.get('content', result.get('text', '')) for result in search_results]
                        
                        context_relevancy = self.calculate_context_relevancy(question, contexts)
                        answer_relevancy = self.calculate_answer_relevancy(question, expected_answer, contexts)
                        accuracy = self.calculate_accuracy_score(contexts, expected_contexts)
                        
                        total_accuracy += accuracy
                        total_context_rel += context_relevancy
                        total_answer_rel += answer_relevancy
                        successful_queries += 1
                        
                        logger.info(f"  ì‹¤í–‰ ì§ˆë¬¸ {idx}: ì •í™•ë„={accuracy:.3f}")
                        
                except Exception as e:
                    logger.warning(f"  ì‹¤í–‰ ì§ˆë¬¸ {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        
        # í‰ê·  ê³„ì‚°
        if successful_queries > 0:
            avg_accuracy = total_accuracy / successful_queries
            avg_context_rel = total_context_rel / successful_queries
            avg_answer_rel = total_answer_rel / successful_queries
        else:
            avg_accuracy = avg_context_rel = avg_answer_rel = 0.0
        
        return {
            'accuracy_score': avg_accuracy,
            'context_relevancy': avg_context_rel,
            'answer_relevancy': avg_answer_rel,
            'additional_metrics': {
                'successful_queries': successful_queries,
                'total_queries': len(action_indices),
                'question_type': 'action_oriented',
                'top_k_used': 6
            }
        }
    
    def scenario_04_performance_stress_test(self) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ 4: ì„±ëŠ¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (ì—°ì† ì§ˆì˜)"""
        # ëª¨ë“  ì§ˆë¬¸ì„ ë¹ ë¥´ê²Œ ì—°ì† ì²˜ë¦¬
        total_response_time = 0
        successful_queries = 0
        failed_queries = 0
        response_times = []
        
        for idx, question in enumerate(self.amnesty_data['questions'][:5]):  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
            try:
                start_time = time.time()
                search_results = self.retriever.retrieve(query=question, top_k=3)
                response_time = time.time() - start_time
                
                if search_results and len(search_results) > 0:
                    successful_queries += 1
                    total_response_time += response_time
                    response_times.append(response_time)
                    logger.info(f"  ìŠ¤íŠ¸ë ˆìŠ¤ ì§ˆë¬¸ {idx}: ì‘ë‹µì‹œê°„={response_time:.3f}ì´ˆ")
                else:
                    failed_queries += 1
                    
            except Exception as e:
                failed_queries += 1
                logger.warning(f"  ìŠ¤íŠ¸ë ˆìŠ¤ ì§ˆë¬¸ {idx} ì‹¤íŒ¨: {str(e)}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_response_time = total_response_time / successful_queries if successful_queries > 0 else 0
        success_rate = successful_queries / (successful_queries + failed_queries) if (successful_queries + failed_queries) > 0 else 0
        
        # ì„±ëŠ¥ ê¸°ì¤€: í‰ê·  ì‘ë‹µì‹œê°„ < 2ì´ˆ, ì„±ê³µë¥  > 90%
        performance_score = min(success_rate, 1.0 - (avg_response_time / 2.0)) if avg_response_time < 2.0 else 0.0
        
        return {
            'accuracy_score': performance_score,
            'context_relevancy': success_rate,
            'answer_relevancy': 1.0 - (avg_response_time / 2.0) if avg_response_time < 2.0 else 0.0,
            'additional_metrics': {
                'successful_queries': successful_queries,
                'failed_queries': failed_queries,
                'avg_response_time': avg_response_time,
                'max_response_time': max(response_times) if response_times else 0,
                'min_response_time': min(response_times) if response_times else 0,
                'test_type': 'performance_stress'
            }
        }
    
    def load_test_data(self) -> bool:
        """amnesty_qa í‰ê°€ ë°ì´í„° ë¡œë“œ"""
        try:
            data_path = Path("data/amnesty_qa/amnesty_qa_evaluation.json")
            
            if not data_path.exists():
                raise FileNotFoundError(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_path}")
            
            with open(data_path, 'r', encoding='utf-8') as f:
                self.amnesty_data = json.load(f)
            
            # ë°ì´í„° êµ¬ì¡° ê²€ì¦
            required_fields = ['questions', 'contexts', 'ground_truths']
            for field in required_fields:
                if field not in self.amnesty_data:
                    raise KeyError(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
            
            self.test_data_info = {
                'total_questions': len(self.amnesty_data['questions']),
                'data_source': str(data_path),
                'load_time': datetime.now().isoformat()
            }
            
            logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.amnesty_data['questions'])}ê°œ ì§ˆë¬¸")
            return True
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def run_scenario_test(self, scenario_name: str, test_function, expected_accuracy: float = 0.8) -> ScenarioResult:
        """ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info(f"\n--- ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸: {scenario_name} ---")
        start_time = time.time()
        
        try:
            # ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            test_result = test_function()
            response_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            if test_result and isinstance(test_result, dict):
                accuracy = test_result.get('accuracy_score', 0.0)
                context_relevancy = test_result.get('context_relevancy', 0.0)
                answer_relevancy = test_result.get('answer_relevancy', 0.0)
                
                # ì„±ê³µ ê¸°ì¤€ íŒë‹¨
                success = (
                    accuracy >= expected_accuracy and
                    context_relevancy >= 0.8 and
                    answer_relevancy >= 0.75
                )
                
                result = ScenarioResult(
                    scenario_name=scenario_name,
                    success=success,
                    accuracy_score=accuracy,
                    context_relevancy=context_relevancy,
                    answer_relevancy=answer_relevancy,
                    response_time=response_time,
                    additional_metrics=test_result.get('additional_metrics', {})
                )
                
                status = "âœ“" if success else "âœ—"
                logger.info(f"{status} {scenario_name}: ì •í™•ë„={accuracy:.3f}, ë¬¸ë§¥ê´€ë ¨ì„±={context_relevancy:.3f}, ë‹µë³€ê´€ë ¨ì„±={answer_relevancy:.3f}, ì‹œê°„={response_time:.3f}ì´ˆ")
                
                return result
            else:
                raise Exception("í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ì—ì„œ ìœ íš¨í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ")
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = str(e)
            
            result = ScenarioResult(
                scenario_name=scenario_name,
                success=False,
                accuracy_score=0.0,
                context_relevancy=0.0,
                answer_relevancy=0.0,
                response_time=response_time,
                error_message=error_msg
            )
            
            logger.error(f"âœ— {scenario_name} ì‹¤íŒ¨: {error_msg}")
            return result
    
    def scenario_01_basic_factual_questions(self) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸°ë³¸ ì‚¬ì‹¤ ì§ˆë¬¸ ì²˜ë¦¬"""
        # ì‚¬ì‹¤ì  ì§ˆë¬¸ ì„ íƒ (ì •ì˜, ê¸°ë³¸ ì›ë¦¬ ë“±)
        factual_indices = [0, 3, 4, 6]  # ê¸°ë³¸ ì›ë¦¬, ì‹œë¯¼ì •ì¹˜ê¶Œ, ê²½ì œì‚¬íšŒë¬¸í™”ê¶Œ, ì„¸ê³„ì¸ê¶Œì„ ì–¸
        
        total_accuracy = 0
        total_context_rel = 0
        total_answer_rel = 0
        successful_queries = 0
        
        for idx in factual_indices:
            if idx < len(self.amnesty_data['questions']):
                question = self.amnesty_data['questions'][idx]
                expected_contexts = self.amnesty_data['contexts'][idx]
                expected_answer = self.amnesty_data['ground_truths'][idx][0]
                
                try:
                    # ê²€ìƒ‰ ì‹¤í–‰
                    search_results = self.retriever.retrieve(query=question, top_k=5)
                    
                    if search_results and len(search_results) > 0:
                        # RAGAS ë©”íŠ¸ë¦­ ê³„ì‚°
                        contexts = [result.get('content', result.get('text', '')) for result in search_results]
                        
                        # ë‹¨ìˆœí™”ëœ ë©”íŠ¸ë¦­ ê³„ì‚°
                        context_relevancy = self.calculate_context_relevancy(question, contexts)
                        answer_relevancy = self.calculate_answer_relevancy(question, expected_answer, contexts)
                        accuracy = self.calculate_accuracy_score(contexts, expected_contexts)
                        
                        total_accuracy += accuracy
                        total_context_rel += context_relevancy
                        total_answer_rel += answer_relevancy
                        successful_queries += 1
                        
                        logger.info(f"  ì‚¬ì‹¤ ì§ˆë¬¸ {idx}: ì •í™•ë„={accuracy:.3f}")
                        
                except Exception as e:
                    logger.warning(f"  ì‚¬ì‹¤ ì§ˆë¬¸ {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        
        # í‰ê·  ê³„ì‚°
        if successful_queries > 0:
            avg_accuracy = total_accuracy / successful_queries
            avg_context_rel = total_context_rel / successful_queries
            avg_answer_rel = total_answer_rel / successful_queries
        else:
            avg_accuracy = avg_context_rel = avg_answer_rel = 0.0
        
        return {
            'accuracy_score': avg_accuracy,
            'context_relevancy': avg_context_rel,
            'answer_relevancy': avg_answer_rel,
            'additional_metrics': {
                'successful_queries': successful_queries,
                'total_queries': len(factual_indices),
                'question_type': 'factual'
            }
        }
    
    def scenario_05_multilingual_readiness_test(self) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ 5: ë‹¤êµ­ì–´ ëŒ€ì‘ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸"""
        # ì˜ì–´ ì´ì™¸ì˜ ì–¸ì–´ë¡œ ë²ˆì—­ëœ ì§ˆë¬¸ë“¤ë¡œ í…ŒìŠ¤íŠ¸
        multilingual_questions = [
            "Â¿CuÃ¡les son los principios bÃ¡sicos de los derechos humanos?",  # ìŠ¤í˜ì¸ì–´
            "Quels sont les principes fondamentaux des droits de l'homme?",  # í”„ë‘ìŠ¤ì–´
            "What is the role of international law in protecting human rights?",  # ì˜ì–´ (ê¸°ì¤€)
            "ì¸ê¶Œì„ ë³´í˜¸í•˜ëŠ” ë° ìˆì–´ êµ­ì œë²•ì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€?"  # í•œêµ­ì–´
        ]
        
        total_accuracy = 0
        successful_queries = 0
        
        for idx, question in enumerate(multilingual_questions):
            try:
                search_results = self.retriever.retrieve(query=question, top_k=5)
                
                if search_results and len(search_results) > 0:
                    # ì˜ì–´ ê¸°ì¤€ ì§ˆë¬¸ì˜ ì˜ˆìƒ ê²°ê³¼ì™€ ë¹„êµ
                    expected_contexts = self.amnesty_data['contexts'][1]  # êµ­ì œë²• ê´€ë ¨ ë¬¸ë§¥
                    contexts = [result.get('content', result.get('text', '')) for result in search_results]
                    
                    accuracy = self.calculate_accuracy_score(contexts, expected_contexts)
                    total_accuracy += accuracy
                    successful_queries += 1
                    
                    logger.info(f"  ë‹¤êµ­ì–´ ì§ˆë¬¸ {idx}: ì •í™•ë„={accuracy:.3f}")
                    
            except Exception as e:
                logger.warning(f"  ë‹¤êµ­ì–´ ì§ˆë¬¸ {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        
        avg_accuracy = total_accuracy / successful_queries if successful_queries > 0 else 0.0
        
        return {
            'accuracy_score': avg_accuracy,
            'context_relevancy': 0.8 if avg_accuracy > 0.6 else 0.5,  # ë‹¤êµ­ì–´ëŠ” ê´€ëŒ€í•œ ê¸°ì¤€
            'answer_relevancy': 0.8 if avg_accuracy > 0.6 else 0.5,
            'additional_metrics': {
                'successful_queries': successful_queries,
                'total_queries': len(multilingual_questions),
                'languages_tested': ['es', 'fr', 'en', 'ko'],
                'test_type': 'multilingual'
            }
        }
    
    def scenario_06_edge_case_queries(self) -> Dict[str, Any]:
        """ì‹œë‚˜ë¦¬ì˜¤ 6: ê·¹ë‹¨ì  ì¼€ì´ìŠ¤ ì§ˆì˜ ì²˜ë¦¬"""
        edge_cases = [
            "",  # ë¹ˆ ì§ˆë¬¸
            "a",  # ë„ˆë¬´ ì§§ì€ ì§ˆë¬¸
            "What is the meaning of life and everything about human rights and international law and global politics and social justice?" * 10,  # ë„ˆë¬´ ê¸´ ì§ˆë¬¸
            "asdfghjkl qwertyuiop",  # ë¬´ì˜ë¯¸í•œ ì§ˆë¬¸
            "human rights" * 50  # ë°˜ë³µì ì¸ ì§ˆë¬¸
        ]
        
        successful_handles = 0
        total_cases = len(edge_cases)
        
        for idx, question in enumerate(edge_cases):
            try:
                search_results = self.retriever.retrieve(query=question, top_k=3)
                
                # ê·¹ë‹¨ì  ì¼€ì´ìŠ¤ì—ì„œë„ ì‹œìŠ¤í…œì´ ê¹¨ì§€ì§€ ì•Šìœ¼ë©´ ì„±ê³µ
                if isinstance(search_results, list):  # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ë˜ë©´ OK
                    successful_handles += 1
                    logger.info(f"  ê·¹ë‹¨ ì¼€ì´ìŠ¤ {idx}: ì •ìƒ ì²˜ë¦¬ë¨")
                    
            except Exception as e:
                logger.warning(f"  ê·¹ë‹¨ ì¼€ì´ìŠ¤ {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        
        success_rate = successful_handles / total_cases
        
        return {
            'accuracy_score': success_rate,
            'context_relevancy': success_rate,
            'answer_relevancy': success_rate,
            'additional_metrics': {
                'successful_handles': successful_handles,
                'total_cases': total_cases,
                'robustness_score': success_rate,
                'test_type': 'edge_cases'
            }
        }
    
    def run_all_scenarios(self) -> Dict[str, Any]:
        """ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("=== ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ RAG í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        scenarios = [
            ("ê¸°ë³¸ ì‚¬ì‹¤ ì§ˆë¬¸ ì²˜ë¦¬", self.scenario_01_basic_factual_questions, 0.7),
            ("ë³µì¡í•œ ë¶„ì„ì  ì§ˆë¬¸ ì²˜ë¦¬", self.scenario_02_complex_analytical_questions, 0.6),
            ("ì‹¤í–‰ ì§€í–¥ì  ì§ˆë¬¸ ì²˜ë¦¬", self.scenario_03_action_oriented_questions, 0.7),
            ("ì„±ëŠ¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸", self.scenario_04_performance_stress_test, 0.8),
            ("ë‹¤êµ­ì–´ ëŒ€ì‘ ëŠ¥ë ¥", self.scenario_05_multilingual_readiness_test, 0.5),
            ("ê·¹ë‹¨ì  ì¼€ì´ìŠ¤ ì²˜ë¦¬", self.scenario_06_edge_case_queries, 0.8)
        ]
        
        all_results = []
        total_scenarios = len(scenarios)
        successful_scenarios = 0
        
        for scenario_name, scenario_func, expected_accuracy in scenarios:
            try:
                result = self.run_scenario_test(scenario_name, scenario_func, expected_accuracy)
                all_results.append(result)
                
                if result.success:
                    successful_scenarios += 1
                    
                # ê²°ê³¼ë¥¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ì— ì¶”ê°€
                self.test_results['scenario_results'].append(result.__dict__)
                
            except Exception as e:
                logger.error(f"ì‹œë‚˜ë¦¬ì˜¤ '{scenario_name}' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                error_result = ScenarioResult(
                    scenario_name=scenario_name,
                    success=False,
                    accuracy_score=0.0,
                    context_relevancy=0.0,
                    answer_relevancy=0.0,
                    response_time=0.0,
                    error_message=str(e)
                )
                all_results.append(error_result)
                self.test_results['scenario_results'].append(error_result.__dict__)
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½ ê³„ì‚°
        overall_success_rate = successful_scenarios / total_scenarios
        avg_accuracy = sum(r.accuracy_score for r in all_results) / len(all_results) if all_results else 0
        avg_context_relevancy = sum(r.context_relevancy for r in all_results) / len(all_results) if all_results else 0
        avg_answer_relevancy = sum(r.answer_relevancy for r in all_results) / len(all_results) if all_results else 0
        avg_response_time = sum(r.response_time for r in all_results) / len(all_results) if all_results else 0
        
        # ì „ì²´ ìš”ì•½ ì €ì¥
        self.test_results['overall_summary'] = {
            'total_scenarios': total_scenarios,
            'successful_scenarios': successful_scenarios,
            'overall_success_rate': overall_success_rate,
            'average_accuracy': avg_accuracy,
            'average_context_relevancy': avg_context_relevancy,
            'average_answer_relevancy': avg_answer_relevancy,
            'average_response_time': avg_response_time,
            'test_completion_time': datetime.now().isoformat()
        }
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
        self.test_results['performance_metrics'] = {
            'total_test_duration': sum(r.response_time for r in all_results),
            'fastest_scenario': min(all_results, key=lambda x: x.response_time).scenario_name if all_results else None,
            'slowest_scenario': max(all_results, key=lambda x: x.response_time).scenario_name if all_results else None,
            'most_accurate_scenario': max(all_results, key=lambda x: x.accuracy_score).scenario_name if all_results else None,
            'least_accurate_scenario': min(all_results, key=lambda x: x.accuracy_score).scenario_name if all_results else None
        }
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë³´ ì €ì¥
        self.test_results['test_data_info'] = self.test_data_info
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info(f"\n=== í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤: {total_scenarios}ê°œ")
        logger.info(f"ì„±ê³µí•œ ì‹œë‚˜ë¦¬ì˜¤: {successful_scenarios}ê°œ")
        logger.info(f"ì „ì²´ ì„±ê³µë¥ : {overall_success_rate:.1%}")
        logger.info(f"í‰ê·  ì •í™•ë„: {avg_accuracy:.3f}")
        logger.info(f"í‰ê·  ë¬¸ë§¥ ê´€ë ¨ì„±: {avg_context_relevancy:.3f}")
        logger.info(f"í‰ê·  ë‹µë³€ ê´€ë ¨ì„±: {avg_answer_relevancy:.3f}")
        logger.info(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_response_time:.3f}ì´ˆ")
        
        # ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
        logger.info(f"\n=== ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ===")
        for result in all_results:
            status = "âœ“ PASS" if result.success else "âœ— FAIL"
            logger.info(f"{status} {result.scenario_name}: "
                       f"ì •í™•ë„={result.accuracy_score:.3f}, "
                       f"ë¬¸ë§¥ê´€ë ¨ì„±={result.context_relevancy:.3f}, "
                       f"ë‹µë³€ê´€ë ¨ì„±={result.answer_relevancy:.3f}, "
                       f"ì‹œê°„={result.response_time:.3f}ì´ˆ")
            
            if result.error_message:
                logger.error(f"   ì˜¤ë¥˜: {result.error_message}")
        
        return self.test_results
    
    def save_results(self, output_file: Optional[str] = None) -> bool:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            if output_file is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = f"evaluation_results/real_world_scenarios_results_{timestamp}.json"
            
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ RAG í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        # í…ŒìŠ¤íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        test_runner = RealWorldScenarioTest()
        
        # ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
        results = test_runner.run_all_scenarios()
        
        # ê²°ê³¼ ì €ì¥
        test_runner.save_results()
        
        # ìµœì¢… ìƒíƒœ í™•ì¸
        overall_success = results['overall_summary']['overall_success_rate'] >= 0.7
        
        if overall_success:
            logger.info("\nğŸ‰ ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ RAG í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            return 0
        else:
            logger.warning("\nâš ï¸ ì¼ë¶€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return 1
            
    except Exception as e:
        logger.error(f"í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")
        logger.error(traceback.format_exc())
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
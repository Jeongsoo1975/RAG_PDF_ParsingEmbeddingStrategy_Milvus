#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Re-ranking ì„±ëŠ¥ ë¹„êµ í‰ê°€ í…ŒìŠ¤íŠ¸
Re-ranking ì ìš© ì „í›„ì˜ ê²€ìƒ‰ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•˜ì—¬ íš¨ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import json
import logging
import statistics
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from unittest.mock import patch, MagicMock

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ.setdefault('LOG_LEVEL', 'INFO')

try:
    from src.utils.config import Config
    from src.rag.retriever import DocumentRetriever
    from src.rag.reranker import DocumentReranker
    from src.utils.logger import get_logger
except ImportError as e:
    print(f"Import error: {e}")
    print("Please ensure you're running from the project root directory")
    sys.exit(1)

logger = get_logger("test_reranking_performance")

class ReRankingPerformanceEvaluator:
    """Re-ranking ì„±ëŠ¥ ë¹„êµ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Optional[Config] = None):
        """ì´ˆê¸°í™”"""
        self.config = config or Config()
        self.logger = logger
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.results_dir = Path("evaluation_results")
        self.results_dir.mkdir(exist_ok=True)
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ (í•œêµ­ì–´ ë³´í—˜ ê´€ë ¨)
        self.test_queries = [
            "CIë³´í—˜ê¸ˆì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì„ ì§€ê¸‰ì„œë¹„ìŠ¤íŠ¹ì•½ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "ì œ1ë³´í—˜ê¸°ê°„ê³¼ ì œ2ë³´í—˜ê¸°ê°„ì˜ ì°¨ì´ì ì€?",
            "ë¬´ë°°ë‹¹ ì •ê¸°íŠ¹ì•½ ë³´í—˜ë£Œ ë‚©ì… ë©´ì œ ì¡°ê±´",
            "ì œ18ì¡°ì— ëª…ì‹œëœ ë³´í—˜ê¸ˆì˜ ì¢…ë¥˜",
            "ë³´í—˜ë£Œ ë‚©ì… ë©´ì œ ì‚¬ìœ ",
            "CIë³´í—˜ê¸ˆê³¼ ì„ ì§€ê¸‰ì„œë¹„ìŠ¤íŠ¹ì•½ì˜ ì°¨ì´",
            "ì¤‘ëŒ€í•œ ì§ˆë³‘ì— í•´ë‹¹í•˜ëŠ” ì¡°ê±´",
            "ë³´í—˜ê³„ì•½ í•´ì§€ ì‹œ í™˜ê¸‰ê¸ˆ",
            "ë³´ì¥ê°œì‹œì¼ì€ ì–¸ì œë¶€í„°ì¸ê°€ìš”?"
        ]
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥ì†Œ
        self.performance_metrics = {
            "with_reranking": {},
            "without_reranking": {},
            "comparison": {}
        }
        
    def setup_retrievers(self) -> Tuple[DocumentRetriever, DocumentRetriever]:
        """Re-ranking ì ìš©/ë¯¸ì ìš© retriever ì„¤ì •"""
        self.logger.info("Setting up retrievers for performance comparison...")
        
        # Re-ranking í™œì„±í™”ëœ retriever
        config_with_rerank = Config()
        config_with_rerank.retrieval = {
            'top_k': 5,
            'similarity_threshold': 0.3,
            'reranking': True,
            'reranking_batch_size': 8,
            'offline_mode': True,  # ì¼ê´€ëœ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì‚¬ìš©
            'hybrid_search': False  # ìˆœìˆ˜ vector + reranking í…ŒìŠ¤íŠ¸
        }
        
        # Re-ranking ë¹„í™œì„±í™”ëœ retriever
        config_without_rerank = Config()
        config_without_rerank.retrieval = {
            'top_k': 5,
            'similarity_threshold': 0.3,
            'reranking': False,
            'offline_mode': True,
            'hybrid_search': False
        }
        
        retriever_with_rerank = DocumentRetriever(config_with_rerank)
        retriever_without_rerank = DocumentRetriever(config_without_rerank)
        
        return retriever_with_rerank, retriever_without_rerank
    
    def measure_search_performance(self, retriever: DocumentRetriever, query: str, 
                                  test_name: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •"""
        start_time = time.time()
        
        try:
            # ê²€ìƒ‰ ì‹¤í–‰
            results = retriever.retrieve(
                query=query,
                top_k=5,
                use_parent_chunks=False,
                enable_query_optimization=True
            )
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            # ê²°ê³¼ ë¶„ì„
            result_analysis = {
                "query": query,
                "test_name": test_name,
                "execution_time": execution_time,
                "result_count": len(results),
                "results": []
            }
            
            # ê° ê²°ê³¼ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            for i, result in enumerate(results):
                result_info = {
                    "rank": i + 1,
                    "id": result.get("id", f"unknown_{i}"),
                    "similarity": result.get("similarity", 0.0),
                    "rerank_score": result.get("rerank_score", None),
                    "hybrid_score": result.get("hybrid_score", None),
                    "content_preview": result.get("content", "")[:100] + "...",
                    "chunk_type": result.get("metadata", {}).get("chunk_type", "unknown")
                }
                result_analysis["results"].append(result_info)
            
            self.logger.debug(f"{test_name} - Query: '{query}' completed in {execution_time:.4f}s")
            return result_analysis
            
        except Exception as e:
            self.logger.error(f"Error in {test_name} for query '{query}': {e}")
            return {
                "query": query,
                "test_name": test_name,
                "execution_time": -1,
                "error": str(e),
                "result_count": 0,
                "results": []
            }
    
    def compare_ranking_changes(self, results_with: List[Dict], results_without: List[Dict]) -> Dict[str, Any]:
        """ëœí‚¹ ë³€í™” ë¶„ì„"""
        if not results_with or not results_without:
            return {"error": "One or both result sets are empty"}
        
        # ID ê¸°ë°˜ ëœí‚¹ ë¹„êµ
        ids_with = [r.get("id") for r in results_with]
        ids_without = [r.get("id") for r in results_without]
        
        # ìˆœìœ„ ë³€í™” ê³„ì‚°
        rank_changes = []
        position_improvements = 0
        position_degradations = 0
        
        for i, id_with in enumerate(ids_with):
            if id_with in ids_without:
                old_rank = ids_without.index(id_with) + 1
                new_rank = i + 1
                rank_change = old_rank - new_rank  # ì–‘ìˆ˜ë©´ ìˆœìœ„ ìƒìŠ¹
                
                rank_changes.append({
                    "id": id_with,
                    "old_rank": old_rank,
                    "new_rank": new_rank,
                    "rank_change": rank_change
                })
                
                if rank_change > 0:
                    position_improvements += 1
                elif rank_change < 0:
                    position_degradations += 1
        
        # ìƒˆë¡œ ë‚˜íƒ€ë‚œ ê²°ê³¼ (Re-rankingìœ¼ë¡œ ì¸í•´ ëŒ€ì²´ëœ ê²°ê³¼)
        new_results = [id_with for id_with in ids_with if id_with not in ids_without]
        
        # ì‚¬ë¼ì§„ ê²°ê³¼
        removed_results = [id_without for id_without in ids_without if id_without not in ids_with]
        
        return {
            "total_compared": len(rank_changes),
            "position_improvements": position_improvements,
            "position_degradations": position_degradations,
            "position_unchanged": len(rank_changes) - position_improvements - position_degradations,
            "new_results_count": len(new_results),
            "removed_results_count": len(removed_results),
            "rank_changes": rank_changes,
            "new_results": new_results,
            "removed_results": removed_results,
            "improvement_rate": position_improvements / len(rank_changes) if rank_changes else 0.0
        }
    
    def run_performance_comparison(self) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("=== Re-ranking ì„±ëŠ¥ ë¹„êµ í‰ê°€ ì‹œì‘ ===\n")
        
        # Retriever ì„¤ì •
        retriever_with_rerank, retriever_without_rerank = self.setup_retrievers()
        
        # ê° ì¿¼ë¦¬ë³„ ì„±ëŠ¥ ë¹„êµ
        query_results = []
        overall_stats = {
            "total_queries": len(self.test_queries),
            "successful_comparisons": 0,
            "failed_comparisons": 0,
            "execution_times": {"with_reranking": [], "without_reranking": []},
            "improvement_rates": [],
            "position_changes": {"improvements": 0, "degradations": 0, "unchanged": 0}
        }
        
        for i, query in enumerate(self.test_queries, 1):
            self.logger.info(f"\n[{i}/{len(self.test_queries)}] í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{query}'")
            
            # Re-ranking ì ìš©í•œ ê²€ìƒ‰
            self.logger.info("  â†’ Re-ranking ì ìš© ê²€ìƒ‰ ì‹¤í–‰...")
            results_with = self.measure_search_performance(
                retriever_with_rerank, query, "WITH_RERANKING"
            )
            
            # Re-ranking ë¯¸ì ìš© ê²€ìƒ‰
            self.logger.info("  â†’ Re-ranking ë¯¸ì ìš© ê²€ìƒ‰ ì‹¤í–‰...")
            results_without = self.measure_search_performance(
                retriever_without_rerank, query, "WITHOUT_RERANKING"
            )
            
            # ê²°ê³¼ ë¹„êµ ë¶„ì„
            if (results_with.get("execution_time", -1) > 0 and 
                results_without.get("execution_time", -1) > 0 and
                results_with.get("result_count", 0) > 0 and
                results_without.get("result_count", 0) > 0):
                
                # ë­í‚¹ ë³€í™” ë¶„ì„
                ranking_comparison = self.compare_ranking_changes(
                    results_with["results"], results_without["results"]
                )
                
                # ì‹¤í–‰ ì‹œê°„ ë¹„êµ
                time_with = results_with["execution_time"]
                time_without = results_without["execution_time"]
                time_overhead = time_with - time_without
                time_ratio = time_with / time_without if time_without > 0 else float('inf')
                
                query_result = {
                    "query": query,
                    "query_index": i,
                    "results_with_reranking": results_with,
                    "results_without_reranking": results_without,
                    "ranking_comparison": ranking_comparison,
                    "performance_metrics": {
                        "execution_time_with": time_with,
                        "execution_time_without": time_without,
                        "time_overhead": time_overhead,
                        "time_ratio": time_ratio,
                        "improvement_rate": ranking_comparison.get("improvement_rate", 0.0)
                    }
                }
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                overall_stats["successful_comparisons"] += 1
                overall_stats["execution_times"]["with_reranking"].append(time_with)
                overall_stats["execution_times"]["without_reranking"].append(time_without)
                overall_stats["improvement_rates"].append(ranking_comparison.get("improvement_rate", 0.0))
                
                # ìœ„ì¹˜ ë³€í™” í†µê³„
                overall_stats["position_changes"]["improvements"] += ranking_comparison.get("position_improvements", 0)
                overall_stats["position_changes"]["degradations"] += ranking_comparison.get("position_degradations", 0)
                overall_stats["position_changes"]["unchanged"] += ranking_comparison.get("position_unchanged", 0)
                
                # ë¡œê·¸ ì¶œë ¥
                self.logger.info(f"  âœ“ ì‹¤í–‰ ì‹œê°„: {time_with:.4f}s (Re-ranking) vs {time_without:.4f}s (ê¸°ë³¸)")
                self.logger.info(f"  âœ“ ì‹œê°„ ì˜¤ë²„í—¤ë“œ: +{time_overhead:.4f}s ({time_ratio:.2f}ë°°)")
                self.logger.info(f"  âœ“ ìˆœìœ„ ê°œì„ ìœ¨: {ranking_comparison.get('improvement_rate', 0.0):.2%}")
                self.logger.info(f"  âœ“ ìˆœìœ„ ë³€í™”: ê°œì„  {ranking_comparison.get('position_improvements', 0)}, "
                                f"í•˜ë½ {ranking_comparison.get('position_degradations', 0)}, "
                                f"ìœ ì§€ {ranking_comparison.get('position_unchanged', 0)}")
                
            else:
                query_result = {
                    "query": query,
                    "query_index": i,
                    "error": "One or both searches failed",
                    "results_with_reranking": results_with,
                    "results_without_reranking": results_without
                }
                overall_stats["failed_comparisons"] += 1
                self.logger.warning(f"  âœ— ì¿¼ë¦¬ '{query}' ë¹„êµ ì‹¤íŒ¨")
            
            query_results.append(query_result)
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        overall_summary = self.calculate_overall_statistics(overall_stats)
        
        return {
            "test_info": {
                "test_name": "Re-ranking Performance Comparison",
                "timestamp": datetime.now().isoformat(),
                "total_queries": len(self.test_queries),
                "test_queries": self.test_queries
            },
            "query_results": query_results,
            "overall_statistics": overall_stats,
            "overall_summary": overall_summary
        }
    
    def calculate_overall_statistics(self, overall_stats: Dict[str, Any]) -> Dict[str, Any]:
        """ì „ì²´ í†µê³„ ê³„ì‚°"""
        summary = {}
        
        # ì„±ê³µë¥ 
        total_queries = overall_stats["total_queries"]
        successful = overall_stats["successful_comparisons"]
        failed = overall_stats["failed_comparisons"]
        
        summary["success_rate"] = successful / total_queries if total_queries > 0 else 0.0
        summary["failure_rate"] = failed / total_queries if total_queries > 0 else 0.0
        
        # ì‹¤í–‰ ì‹œê°„ í†µê³„
        times_with = overall_stats["execution_times"]["with_reranking"]
        times_without = overall_stats["execution_times"]["without_reranking"]
        
        if times_with and times_without:
            summary["avg_time_with_reranking"] = statistics.mean(times_with)
            summary["avg_time_without_reranking"] = statistics.mean(times_without)
            summary["median_time_with_reranking"] = statistics.median(times_with)
            summary["median_time_without_reranking"] = statistics.median(times_without)
            
            summary["avg_time_overhead"] = summary["avg_time_with_reranking"] - summary["avg_time_without_reranking"]
            summary["avg_time_ratio"] = summary["avg_time_with_reranking"] / summary["avg_time_without_reranking"]
            
            # ì‹œê°„ ì˜¤ë²„í—¤ë“œ ë°±ë¶„ìœ¨
            summary["time_overhead_percentage"] = (summary["avg_time_overhead"] / summary["avg_time_without_reranking"]) * 100
        else:
            summary.update({
                "avg_time_with_reranking": 0.0,
                "avg_time_without_reranking": 0.0,
                "median_time_with_reranking": 0.0,
                "median_time_without_reranking": 0.0,
                "avg_time_overhead": 0.0,
                "avg_time_ratio": 0.0,
                "time_overhead_percentage": 0.0
            })
        
        # ìˆœìœ„ ê°œì„  í†µê³„
        improvement_rates = overall_stats["improvement_rates"]
        if improvement_rates:
            summary["avg_improvement_rate"] = statistics.mean(improvement_rates)
            summary["median_improvement_rate"] = statistics.median(improvement_rates)
            summary["max_improvement_rate"] = max(improvement_rates)
            summary["min_improvement_rate"] = min(improvement_rates)
        else:
            summary.update({
                "avg_improvement_rate": 0.0,
                "median_improvement_rate": 0.0,
                "max_improvement_rate": 0.0,
                "min_improvement_rate": 0.0
            })
        
        # ìˆœìœ„ ë³€í™” í†µê³„
        position_changes = overall_stats["position_changes"]
        total_positions = sum(position_changes.values())
        
        if total_positions > 0:
            summary["position_improvement_rate"] = position_changes["improvements"] / total_positions
            summary["position_degradation_rate"] = position_changes["degradations"] / total_positions
            summary["position_unchanged_rate"] = position_changes["unchanged"] / total_positions
        else:
            summary.update({
                "position_improvement_rate": 0.0,
                "position_degradation_rate": 0.0,
                "position_unchanged_rate": 0.0
            })
        
        # ì„±ëŠ¥ í‰ê°€
        summary["performance_verdict"] = self.evaluate_performance(summary)
        
        return summary
    
    def evaluate_performance(self, summary: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ í‰ê°€ ë° ê²°ë¡  ë„ì¶œ"""
        verdict = {
            "overall_rating": "Unknown",
            "improvement_assessment": "Unknown",
            "time_overhead_assessment": "Unknown",
            "recommendation": "More testing needed"
        }
        
        # ìˆœìœ„ ê°œì„  í‰ê°€ (ê¸°ì¤€: 10% ì´ìƒ)
        avg_improvement = summary.get("avg_improvement_rate", 0.0)
        if avg_improvement >= 0.15:  # 15% ì´ìƒ
            verdict["improvement_assessment"] = "Excellent"
        elif avg_improvement >= 0.10:  # 10% ì´ìƒ
            verdict["improvement_assessment"] = "Good"
        elif avg_improvement >= 0.05:  # 5% ì´ìƒ
            verdict["improvement_assessment"] = "Moderate"
        else:
            verdict["improvement_assessment"] = "Poor"
        
        # ì‹œê°„ ì˜¤ë²„í—¤ë“œ í‰ê°€ (ê¸°ì¤€: 2ë°° ì´ë‚´)
        time_ratio = summary.get("avg_time_ratio", 1.0)
        if time_ratio <= 1.5:  # 1.5ë°° ì´ë‚´
            verdict["time_overhead_assessment"] = "Excellent"
        elif time_ratio <= 2.0:  # 2ë°° ì´ë‚´
            verdict["time_overhead_assessment"] = "Good"
        elif time_ratio <= 3.0:  # 3ë°° ì´ë‚´
            verdict["time_overhead_assessment"] = "Moderate"
        else:
            verdict["time_overhead_assessment"] = "Poor"
        
        # ì¢…í•© í‰ê°€
        improvement_score = {"Excellent": 4, "Good": 3, "Moderate": 2, "Poor": 1}[verdict["improvement_assessment"]]
        time_score = {"Excellent": 4, "Good": 3, "Moderate": 2, "Poor": 1}[verdict["time_overhead_assessment"]]
        
        overall_score = (improvement_score * 0.7) + (time_score * 0.3)  # ê°œì„ ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        
        if overall_score >= 3.5:
            verdict["overall_rating"] = "Excellent"
            verdict["recommendation"] = "Re-ranking ê¸°ëŠ¥ì„ í™œì„±í™”í•˜ëŠ” ê²ƒì„ ê°•ë ¥íˆ ì¶”ì²œí•©ë‹ˆë‹¤."
        elif overall_score >= 2.5:
            verdict["overall_rating"] = "Good"
            verdict["recommendation"] = "Re-ranking ê¸°ëŠ¥ì„ í™œì„±í™”í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤."
        elif overall_score >= 1.5:
            verdict["overall_rating"] = "Moderate"
            verdict["recommendation"] = "Re-ranking ê¸°ëŠ¥ì˜ ì„¤ì •ì„ ìµœì í™”í•œ í›„ ì‚¬ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”."
        else:
            verdict["overall_rating"] = "Poor"
            verdict["recommendation"] = "Re-ranking ê¸°ëŠ¥ì˜ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        return verdict
    
    def save_results(self, results: Dict[str, Any]) -> str:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"reranking_performance_{timestamp}.json"
        filepath = self.results_dir / filename
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"âœ“ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            return str(filepath)
            
        except Exception as e:
            self.logger.error(f"âœ— ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def print_summary_report(self, results: Dict[str, Any]):
        """ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥"""
        summary = results.get("overall_summary", {})
        verdict = summary.get("performance_verdict", {})
        
        self.logger.info("\n" + "="*60)
        self.logger.info("         Re-ranking ì„±ëŠ¥ ë¹„êµ í‰ê°€ ê²°ê³¼")
        self.logger.info("="*60)
        
        # ê¸°ë³¸ ì •ë³´
        test_info = results.get("test_info", {})
        self.logger.info(f"í…ŒìŠ¤íŠ¸ ì¼ì‹œ: {test_info.get('timestamp', 'Unknown')}")
        self.logger.info(f"ì´ ì¿¼ë¦¬ ìˆ˜: {test_info.get('total_queries', 0)}ê°œ")
        
        # ì„±ê³µë¥ 
        self.logger.info(f"ì„±ê³µë¥ : {summary.get('success_rate', 0.0):.2%}")
        
        # ì‹¤í–‰ ì‹œê°„ ë¹„êµ
        self.logger.info("\nâ±ï¸  ì‹¤í–‰ ì‹œê°„ ë¹„êµ:")
        self.logger.info(f"  - Re-ranking ì ìš©: {summary.get('avg_time_with_reranking', 0.0):.4f}s (í‰ê· )")
        self.logger.info(f"  - Re-ranking ë¯¸ì ìš©: {summary.get('avg_time_without_reranking', 0.0):.4f}s (í‰ê· )")
        self.logger.info(f"  - ì‹œê°„ ì˜¤ë²„í—¤ë“œ: +{summary.get('avg_time_overhead', 0.0):.4f}s ({summary.get('avg_time_ratio', 1.0):.2f}ë°°)")
        self.logger.info(f"  - ì˜¤ë²„í—¤ë“œ ë¹„ìœ¨: {summary.get('time_overhead_percentage', 0.0):.1f}%")
        
        # ìˆœìœ„ ê°œì„  ë¹„êµ
        self.logger.info("\nğŸ“ˆ ìˆœìœ„ ê°œì„  íš¨ê³¼:")
        self.logger.info(f"  - í‰ê·  ê°œì„ ìœ¨: {summary.get('avg_improvement_rate', 0.0):.2%}")
        self.logger.info(f"  - ìµœëŒ€ ê°œì„ ìœ¨: {summary.get('max_improvement_rate', 0.0):.2%}")
        self.logger.info(f"  - ìœ„ì¹˜ ë³€í™” ë¹„ìœ¨:")
        self.logger.info(f"    â€¢ ê°œì„ : {summary.get('position_improvement_rate', 0.0):.2%}")
        self.logger.info(f"    â€¢ í•˜ë½: {summary.get('position_degradation_rate', 0.0):.2%}")
        self.logger.info(f"    â€¢ ìœ ì§€: {summary.get('position_unchanged_rate', 0.0):.2%}")
        
        # ì„±ëŠ¥ í‰ê°€
        self.logger.info("\nğŸ† ì„±ëŠ¥ í‰ê°€:")
        self.logger.info(f"  - ì „ì²´ í‰ê°€: {verdict.get('overall_rating', 'Unknown')}")
        self.logger.info(f"  - ê°œì„  íš¨ê³¼: {verdict.get('improvement_assessment', 'Unknown')}")
        self.logger.info(f"  - ì‹œê°„ ì˜¤ë²„í—¤ë“œ: {verdict.get('time_overhead_assessment', 'Unknown')}")
        
        # ì¶”ì²œ ì‚¬í•­
        self.logger.info("\nğŸ“ ì¶”ì²œ ì‚¬í•­:")
        self.logger.info(f"  {verdict.get('recommendation', 'More analysis needed')}")
        
        self.logger.info("\n" + "="*60)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('logs/test_reranking_performance.log', encoding='utf-8')
        ]
    )
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path('logs')
    log_dir.mkdir(exist_ok=True)
    
    logger.info("Re-ranking ì„±ëŠ¥ ë¹„êµ í‰ê°€ ì‹œì‘")
    
    try:
        # í‰ê°€ì ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        evaluator = ReRankingPerformanceEvaluator()
        
        # ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
        results = evaluator.run_performance_comparison()
        
        # ê²°ê³¼ ì €ì¥
        saved_file = evaluator.save_results(results)
        
        # ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥
        evaluator.print_summary_report(results)
        
        # ìµœì¢… ì„±ê³µ ë©”ì‹œì§€
        overall_rating = results.get("overall_summary", {}).get("performance_verdict", {}).get("overall_rating", "Unknown")
        
        if overall_rating in ["Excellent", "Good"]:
            logger.info(f"\nâœ“ Re-ranking ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ - ê²°ê³¼: {overall_rating}")
            logger.info("Re-ranking ê¸°ëŠ¥ì˜ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤!")
        else:
            logger.info(f"\nâš ï¸ Re-ranking ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ - ê²°ê³¼: {overall_rating}")
            logger.info("Re-ranking ê¸°ëŠ¥ì˜ ì¶”ê°€ ìµœì í™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        if saved_file:
            logger.info(f"ìƒì„¸ ê²°ê³¼ëŠ” {saved_file}ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        logger.error(f"âœ— Re-ranking ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨: {e}", exc_info=True)
        return False


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
